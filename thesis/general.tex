%%%%%% General %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{{{
\chapter{Background}\label{sec:general}

In this chapter, we describe the basics, techniques, tools, and programs used in
this thesis. It provides a general overview that is required to understand the
details of the work in the following chapters. We start by showing how modern
computers execute programs and code. We discuss file formats required to load
programs and how common computing architectures structure their machine code. We
then look at how programs and source code is tested and what new technologies
evolved in that matter. As we deal with Linux-based operating systems, we take a
look at the permission system used by these.

Additionally, we discuss topics in secure communication. We describe the basic
principles of network connections, web servers, and their security. As the
thesis provides automation for searching for bits to flip with Rowhammer, we
also look at this kind of attack, how it works, and discuss microarchitectural
attacks in general.

\section{Executing Programs}

Processing units (PU) execute machine code. Any device running programs contains
at least one PU. Usually, the central processing unit (CPU) runs programs and
triggers other PUs to run tasks if needed. Often referred to as the main
processor, the CPU carries out the instruction given to it by the machine code
representing a program. A PU can only step over single instructions and execute
them one by one. This is done by moving the instruction pointer to different
memory locations. Simple processors may only execute one single program. On
ordinary desktop computers, the operating system is the main program which can
load other programs as processes and manage them. CPUs can only execute machine
code, but programs are usually written in human-readable programming languages
which are then translated to machine language by either a compiler or an
interpreter. We refer to files created by a compiler as executables or binaries.
These files can directly be loaded into memory by the operating system.
Operating Systems use defined structures for executables to make this loading
possible, for example, GNU/Linux systems use the executable and linkable format
(ELF), and Windows uses the portable executable format (PE).

\subsection{CPU Instructions}

Machine code, in general, is a list of instructions, encoded in some binary
format. Usually, an instruction consists of an operation code (opcode), telling
the PU what to do, and parameters for the opcode, which the processor uses for
the operation. Instructions can operate on registers or locations in memory
directly. Available instructions and their design are depending on the
architecture used. The most common instruction set architectures (ISA) are Intel
$x86$, which exists in 32-bit mode (\texttt{x86\_32}), and 64-bit mode
(\texttt{x86\_64} or \texttt{AMD64}), and the ARM ISA which is the architecture
used by most mobile processors.

\begin{table}[!htb]
\centering
\begin{tabular}{ccccccc}
\cline{2-7}
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Prefix} &
\multicolumn{1}{c|}{Opcode} & \multicolumn{1}{c|}{ModR/M} &
\multicolumn{1}{c|}{SIB} & \multicolumn{1}{c|}{Displacement} &
\multicolumn{1}{c|}{Immediate} \\ \cline{2-7}
bytes         & 1 per prefix                & 1, 2 or 3
 & 0 or 1                      & 0 or 1                   & 0, 1, 2 or 4
             & 0, 1, 2 or 4
\end{tabular}
\caption{Instruction Format for Intel 64 and IA-32
architectures~\cite{intelsys}, showing how many different instruction lengths
are possible in modern architectures and also which parts they may contain.}
\label{tab:instrfor}
\end{table}

Table~\ref{tab:instrfor} shows the instruction format for Intel 64 and IA-32
Architectures. An instruction is divided into six possible parts, which all have
different functionality for the CPU executing it. The parts are described in
detail by the Intel manual~\cite{intelsys} Section 2.1.

\subsubsection{Prefix}

The instruction prefix can be used to change the behaviour of the CPU for the
following opcode. It can be used as a lock (\texttt{0xF0}), which ensures shared
memory is protected between multiple processing cores. It also can be used as a
prefix for repeating an instruction (\texttt{0xF2} - repeat-not zero or
\texttt{0xF3} - repeat-if-zero), which could be used for strings or
I/O-operations. Besides repeat, the \texttt{0xF2} prefix is also mandatory for
some instructions, such as \texttt{CPUID}. The prefix can also indicate a larger
operand-size by \texttt{0x66} or a larger address-size by \texttt{0x67}. This
address-size prefix provides programs with the ability to switch between 16-bit
and 32-bit addressing modes. There are also branch-hint prefixes (\texttt{0x2E}
- branch not taken, \texttt{0x3E} - branch taken), but according to the Intel
manual~\cite{intelsys}, their usage is deprecated for modern architectures.

\subsubsection{Streaming SIMD Extensions (SSE)}

``Single instruction multiple data''-instructions use the \texttt{0x66} prefix
to indicate a particular behaviour of the CPU. The instruction can perform the
same operation on multiple data pointers at the same time. These instructions
are often used in signal processing or graphics processing. Registers used with
SIMD contain mostly with single precision floating point data. According to the
Intel manual~\cite{intelsys} Section 5.5, CPUs supporting this feature provide
at least eight SIMD registers, namely \texttt{XMM0} to \texttt{XMM7}. Newer
generations of Intel CPUs might also provide double-precision floating point,
64-bit registers.

\subsubsection{Opcode}

Opcodes describe what the instruction does, if it needs parameters, like
registers or addresses to physical memory. An opcode is usually one byte long,
there exist two-byte opcodes, which would be declared by a prefix. The primary
opcode also defines so-called fields, which are used to declare the direction of
operation, size of displacements, register encoding, or sign extension. The
\texttt{ModR/M} byte, therefore, also holds a 3-bit opcode field.

\subsubsection{ModR/M and SIB Bytes}

The ModR/M byte is split into three parts. First is the mod field, it is
combined with the r/m field to provide eight registers and twenty-four
addressing modes. Second is the reg/opcode field. It is used to provide a
register number or three additional bits for the opcode. The r/m field is either
used in combination with the mod field to name addressing modes, or it specifies
a register as an operand for the opcode.

Some addressing modes given by the ModR/M byte require a second byte of
information. This information is given by the Scale Index Base (SIB) byte. The
CPU uses this byte as follows: 3 bits give the base, 3 bits give the index, and
the last 2 bits give the scale. To get the address the value stored in the index
register is multiplied with the scale and added to the value stored in the base
register.

\subsubsection{Displacement and Immediate Data}

Besides using the ModR/M and SIB bytes for addressing, the mod field can also
state to use a so-called displacement as an additional address offset. The
calculated address from the SIB byte is then added to the value in the
displacement. The displacement can have a size between zero and four bytes.

Some operands may use data directly provided by the instruction instead of
loading it from memory or a register. Such data is given in the immediate field
of an instruction. Most arithmetic instructions allow immediate data to be
passed, but also instructions like \texttt{mov} allow such parameters. The usage
of immediate data is declared by the ModR/M byte.

\subsection{Executable and Linkable Format (ELF)}

ELF is an object file format. It is used to describe a program and gives
instructions on how to load it into memory to make it ready for execution
by the processor without applying changes to the binary-content itself. A
linker creates ELF files after an assembler turned the program\textquotesingle s
source code into machine code. Most modern Linux-based operating systems follow
the ELF specification released by the Tool Interface Standard
Committee~\cite{elfspec}. Therefore, we need to understand the layout of such
executable files before introducing bitflips in them.

\subsubsection{Structure of an ELF file}

\begin{figure}
  \centering
  \begin{tikzpicture}
  \node (eh) [block] {ELF Header};
  \node (ph) [block, right of=eh, xshift=3cm] {Program Header Table};
  \node (sh) [block, below of=eh, yshift=-2cm] {Section Header Table};
  \node (dot0) [dot, below of=ph] {};
  \node (dot1) [dot, below of=dot0, yshift=0.5cm] {};
  \node (text) [block, below of=dot1] {.text};
  \node (rodata) [block, below of=text] {.rodata};
  \node (data) [block, below of=rodata] {.data};
  \node (bss) [block, below of=data] {.bss};
  \node (dot2) [dot, below of=bss] {};
  \node (dot3) [dot, below of=dot2, yshift=0.5cm] {};
  \node (seg02) [segblock, right of=text, xshift=1cm, yshift=-0.5cm] {02};
  \node (dot4) [dot, above of=seg02] {};
  \node (dot5) [dot, above of=dot4, yshift=-0.5cm] {};
  \node (seg03) [segblock, right of=data, xshift=1cm, yshift=-0.5cm] {03};
  \node (dot6) [dot, below of=seg03] {};
  \node (dot7) [dot, below of=dot6, yshift=0.5cm] {};
  % connections
  \path [thick,->,>=stealth] (eh) edge[out=0, in=180] (ph);
  \path [thick,->,>=stealth] (eh) edge[out=180, in=180] (sh);
  \path [thick,->,>=stealth] (sh) edge[out=0, in=180] (text);
  \path [thick,->,>=stealth] (sh) edge[out=0, in=180] (rodata);
  \path [thick,->,>=stealth] (sh) edge[out=0, in=180] (data);
  \path [thick,->,>=stealth] (sh) edge[out=0, in=180] (bss);
  \path [thick,->,>=stealth] (ph) edge[out=0, in=0] (seg02);
  \path [thick,->,>=stealth] (ph) edge[out=0, in=0] (seg03);
  \path [thick,->,>=stealth] (seg02) edge[out=180, in=0] (text);
  \path [thick,->,>=stealth] (seg02) edge[out=180, in=0] (rodata);
  \path [thick,->,>=stealth] (seg03) edge[out=180, in=0] (data);
  \path [thick,->,>=stealth] (seg03) edge[out=180, in=0] (bss);
  \end{tikzpicture}
  \caption{Schematics of the structure of an ELF file showing the four main
sections inside an executable. Sections and segments are connected via the
program header table and the section header table. Segments usually are just
numbers and can refer to multiple sections inside the section header table.}
\label{fig:elfstruct}
\end{figure}

There are two views of an ELF file, the linking view and the execution view.
Those are also called section and segment view respectively. They share the same
ELF header but serve different purposes for the operating system.
Figure~\ref{fig:elfstruct} show the connection between sections and segments via
the two different headers. Sections and segments can be seen as follows.

\subsubsection{Sections}

Sections describe the binary for the linking view. A section can contain
instructions, data, a symbol table and relocation information. Sections reserved
for the system, start with a dot. There can be additional sections defined by
the user. Sections are created and managed by the linker. Sections that are
directly loaded into the program\textquotesingle s memory image are initialized
data (\texttt{.data, .data1}), read-only data (\texttt{.rodata, .rodata1}), and
executable instructions (\texttt{.text}).

\subsubsection{Segments}

Segments describe the virtual memory layout of a loaded binary.
Figure~\ref{fig:elfstruct} shows how segments are used in an ELF file.
Usually, the linker splits segments into different behaviours the loader has to
apply. For example, all read-only data sections are in the same segment. When
loading the image into memory, references inside the ELF file need to be
resolved and loaded into the memory too. After successfully loading the image
and its dependencies the program can be executed.

\subsubsection{Loading ELF files into Memory}

The ELF specification~\cite{elfspec} was released by the Tool Interface Standard
(TIS) in 1995 for UNIX System-V Release 4-based operating systems. Many
operating systems, such as GNU/Linux-based or BSD-based ones, adapted this
standard to execute programs. As we focus on GNU/Linux operating systems, we
look at how to handle ELF files in order to create running programs. Most
operating systems used on computers or mobile phones do not use physical
addresses during the execution of programs, and the operating system is free to
change the position of sections in the virtual address space. Therefore, in an
ELF file, a section only contains a base address and offsets to that address.
For certain compilation settings, the operating system is free to change the
base address in the programs virtual address space.

\subsubsection{Dynamic Linking}

In modern systems, it is common to have multiple functions used by different
programs. Shared libraries can be used to provide such general functions. When
deploying programs, developers usually make sure all required shared libraries
exist on the target platform or ship their software with those included. When
depending on such system libraries, the loader will insert the shared-library
functions into memory at the given entry point. This technique is called dynamic
linking. The loader fetches the information about the required library from the
ELF file and then searches the library paths provided by the operating system
environment to find the library files. If they cannot be found in the given
paths, an error is shown, and the program is not executed.

\section{Analysis and Testing of Executables}

Testing is a significant part of software development. There are many different
approaches and styles for testing. One approach to test a program is to provide
input to it and check if the code operates as expected by validating the output.
With increasing amount of code and an increasing number of bugs found, the style
of testing changed over the years. On the one hand, developers nowadays ideally
provide unit tests inside their code to test single functions and make sure they
work as intended.

On the other hand, sometimes the source code is not available to a tester, or it
is way too much work to read the entire source code to create such tests.
Therefore, developers came up with techniques to test binaries without knowing
the entire source code. For instance, black-box testing is an approach where a
program is tested without knowing its source code.

\subsection{Fuzzing}

Fuzzing is a form of black-box testing, where knowledge about the program itself
is just minimal. Duran~\etal~\cite{randtest} already showed in the 80s that
trying random inputs on programs provides a way to rate its robustness. Tools
applying automatically-generated, pseudo-randomised, inputs to programs are
called fuzzers. Fuzzers attempt to reach corner cases in programs without
knowing which exist and detect undefined behaviour or crashes. Generally,
fuzzers give an overall view on the robustness of a program. They are relatively
easy to apply to programs and can even find bugs usual white-box tests would
miss.

If a fuzzer would only apply random inputs, testing would take a very long time,
as many unnecessary tests would run. Because of this, the technology improved
over time and different fuzzers evolved. For simple programs, just testing
common input mistakes, like negative numbers, newlines, end of file characters,
format strings, or just very long strings might be enough. More advanced
software requires better ways of fuzzing, for example, network protocols, file
systems, or image formats have complex structures which make it less likely to
crash by just random data input. Fuzzers for such kind of software generate
inputs from given examples and apply small changes to them.
Xmlfuzzer~\cite{xmlfuzzer} is a tool crafted specially to test XML schemes in
programs. Besides special format fuzzers, there are more advanced ones which
directly interact with the target, allowing to track execution paths, and
therefore, create even more test cases automatically. ``American Fuzzy Lop''
(AFL), is a fuzzer which is often used to find bugs in open source software,
their website~\cite{aflweb} presents a list of bugs found with it. AFL contains
algorithms which take usual test inputs which would cause successful runs of a
program, parses those inputs and applies random changes to it. These algorithms
allow better code coverage than just generating random input. Another more
advanced fuzzer is Steelix by Yuekang~\etal~\cite{steelix}, which uses
program-state-based binary fuzzing. Steelix applies static analysis and binary
instrumentation to provide more information about the fuzzing process, and
therefore yields better test cases and code coverage.

Shoshitaishvili~\etal~\cite{firmalice} showed that fuzzing can find
authentication bypasses in binary firmware with their tool Firmalice.
Wang~\etal~\cite{inmemfuzzing} showed how in-memory fuzzing can be used to
detect similarities in binary code. This is done by fuzzing every function
available and comparing traces of program behaviours with machine learning
models. They show that they can find similarities in binaries even when
different compilers, and optimisations are used or when obfuscation is applied.

\subsection{Symbolic Execution}

Fuzzing provides a pseudo-random input to a program to test it. Therefore, it
might not cover all possible execution paths of a program. Simplifying a program
to cover all possible execution paths is done in symbolic execution. James C.
King was the first to mentioned this technique in 1976~\cite{symbolexechist}.
Symbolic execution simplifies possible inputs for programs by replacing them
with a logic symbol which could hold fewer possible states than usual data
types. For example, a number can be represented by the following states: maximal
value, minimum value, positive, negative, and zero. The possible symbolic-state
inputs then run through the program to report possible violations of the
program\textquotesingle s specification. King~\cite{symbolexechist} used a
decision tree to cover all possible execution paths in a binary. Modern
frameworks, like \texttt{angr}~\cite{angrpaper}, use different techniques to
analyse a binary.

On the one hand \texttt{angr} can still use a tree structure to cover program
paths, on the other hand, it also provides a fuzzing assisted symbolic execution
technique. AFL is used to provide a crash report, and the symbolic execution
path is used to identify what code sections are reached by the mutation of the
input. By this, the user running the test get better knowledge about where the
program crashes and why it crashes. Besides the symbolic execution of binaries,
\texttt{angr} also allows various other analysis methods such as control-flow
graph recovery, automatic exploit generation or automatic binary hardening.

Stephens~\etal~\cite{driller} showed with Driller how symbolic execution could
be beneficial to fuzzing approaches. Symbolic execution reduces the number of
execution paths to test while covering much code as possible. Driller uses
concolic execution for the path exploring. Concolic execution is the combination
of symbolic and concrete execution of a program.

\subsection{Instrumentation}

Instrumentation comes down to adding new code to already existing one in an
automated manner. Developers use instrumentation as a tool for debugging or
performance measurements by adding calls to timing functions or information
logging to the code. Information from instrumentation could be something like
what functions are called and how often, what execution branches are taken, how
long a subroutine takes, what memory is accessed and many more. It usually also
allows the developer to change the runtime environment of a program, like change
return values or skip instruction calls.

Two different styles of instrumentation exist, one is source instrumentation,
and one is binary instrumentation, where so-called instrumenter calls are either
added to the source of the program before compilation or later added to the ELF
file of the program. The first style can be easier to apply. Furthermore, the
structure of the program is known to the instrumenter. Calls for performance
checks over functions can be done by adding timer calls at the entry and exit
points. However, binary instrumentation also has advantages. For example, users
do not require the source of the program, which might not be available for
proprietary programs. Also, recompilation of the program is not required if the
code for the instrumentation changes. Given enough debug information in the
binary the instrumentation information might not differ much from the
source-based one. With binary instrumentation, it is also possible to change and
read register values before and after specific CPU instructions are executed,
which might not be possible at the source code level, as used registers and
memory areas are unknown to the instrumenter at this point.

\subsubsection{Intel Pin}

Intel Pin~\cite{pintool} is a proprietary, dynamic, binary instrumentation
framework developed and released by Intel, who supply it for free. The framework
provides a large number of API calls which abstracts most of the work done by
Pin and gives easy access to binary analysis. As Pin is a binary instrumentation
framework, the source of the program to be analysed is not needed, and the
program does it need to be recompiled if the instrumentation tool changes. The
framework allows to log and modify the runtime of a program, whereas it is no
problem to log register values or change those before or after desired
instruction calls or even inject code to be run before given calls. Intel Pin
provides a C++ API to write so-called pintools to analyse programs. Besides the
C++ bindings for PIN, also a wrapper for Python exists, namely
\texttt{python-pin}~\cite{pythonpin}.

Luk~\etal~\cite{pintool} describe the system of Pin and how the written pintool
is combined with an application in more detail. When instrumenting a program,
three binaries are used. One is the application to instrument, one is the
compiled pintool, and one is Pin itself. Pin can be seen as a userland virtual
machine. When the instrumentation starts, Pin is loaded into the address space
of the application. Then the \texttt{ptrace} system-call is used to capture the
program\textquotesingle s behaviour. When Pin has initialised itself the tool
overtakes the entry point of the program, or the current program counter if it
was attached to a running application. It then uses its just-in-time (JIT)
compiler to continue the execution. Luk~\etal~\cite{pintool} state that their
approach of loading Pin has various advantages over other instrumentation tools,
which mostly use \texttt{LD\_PRELOAD}, which would not work on statically linked
binaries and also by using an additional library, the address space would be
changed from a normal execution.

The JIT compiler inside Pin is used to inject the instrumentation API into the
application to allow the pintool to use it. The code is loaded into a preserved
code-cache and then executed. Pin always tries to execute as many instructions
as possible without interfering it for debugging. That is why they state,
execution of the code happens one trace at a time. A trace runs either until a
control change happens, such as a branch, a call, or a return instruction, or a
PIN API-call is reached. With this approach, they gain better performance than
other instrumentation techniques.

Luk~\etal~\cite{pintool}, also state that the JIT is not the bottleneck but the
instrumenting code inside the application. That is the reason why Pin optimises
the injected code as far as possible. Pin inlines all analysis functions.
Luk~\etal~\cite{pintool} state that this approach is faster as no calls happen,
which would cause registers to be used for the arguments.
Luk~\etal~\cite{pintool} also state that most other debugging tools use the
\texttt{eflags} CPU registers, which is not ideal, as storing and restoring
these flags is time-consuming, and an extra stack has to be used, as the
original stack is not allowed to be modified. Pin detects in the JIT if the
\texttt{eflag} content is needed later on, and only if it is, saving routines
are called. Another optimisation for their framework is the API call
\texttt{IPOINT\_ANYWHERE}, which specifies that the analysis call can happen
anywhere in the instrumentation context, which allows better optimisation of the
instruction order. Luk~\etal~\cite{pintool} claim, that PIN performs better than
tools like \texttt{valgrind}~\cite{valgrind} and DynamoRIO~\cite{dynrio}.

\section{Permission Model in Linux-based Systems}

The POSIX standard~\cite{posix} defines many interfaces to provide
compatibility between operating systems. It includes a description of
permissions for users and their groups and how they are set. For convenient
reasons, a user can be part of multiple groups. A unique integer ID per user
and group provides identification to the system. The POSIX-defined permission
structure applies to files, which can have properties like readable, writable,
executable. On creation, the user can set these properties separately for the
owner, the group of the owner and others. POSIX also allows users with special
privileges to ignore such permission checks, or change them. Such users are
called superusers.

Besides permissions for files, most Linux-based systems also provide permission
checks for processes and their owners. For example, only processes owned by a
superuser might be allowed to use certain syscalls. In most systems, only one
superuser exists, namely \texttt{root}. Usually, users do not want to login
another time if they need permissions of a superuser for a task. POSIX compliant
operating systems, therefore, often ship tools such as \texttt{su} or
\texttt{sudo}. \texttt{su} stands for switch user and allows a user to change
the current context to another user or execute a command with the other
user\textquotesingle s permissions, providing the correct password of the other
user. \texttt{sudo} includes the same functionality as \texttt{su}.
Additionally, it has a better management system in the background and given the
correct permissions the user does not need to know the other
user\textquotesingle s password to execute commands as the target user.

\subsection{\texttt{setuid} Binary Property}

\texttt{setuid} stands for ``set user identity''~\cite{ogroupsetuid}. It allows
an executable to change its user ID during execution. An additional bit in the
file permissions represents the \texttt{setuid} property, namely
\texttt{S\_ISUID}. This property allows a user to run a program with the
permissions of the file\textquotesingle s owner. Tools like \texttt{su} and
\texttt{sudo} require this permission in order to change the current context. It
is also convenient to allow normal users to use tools like \texttt{ping} or
\texttt{ip}, without requiring them to authenticate as \texttt{root}. We refer
to the GNU libc documentation for more details~\cite{libcpermission}.

While the \texttt{setuid} property might seem convenient, it also brings
security risks. A bug allowing code execution in such a program allows an
attacker to execute that code with the permission of another user. Therefore,
the number of \texttt{setuid} binaries should be low and programs having that
property must be well audited and tested to not jeopardise the
system\textquotesingle s security and integrity.

\subsection{\texttt{chroot}}

\texttt{chroot}~\cite{ogroupchroot} stands for ``change root directory''. One
can either use it directly via a system call or via a wrapper program. A
\texttt{chroot} call changes the root directory for the running process. That is
a change of the environment the program runs in. It is not possible to access
files from outside the new environment nor to interact with processes running in
the outside environment. \texttt{chroot} is either used for testing purposes, to
create packages in clean environments, to check compatibility, as the chrooted
environment could provide a different operating system userland, or for
sandboxing and privilege separation.

\subsection{Pluggable authentication module (PAM)}

PAM is used by most Linux-based operating systems to authenticate users. PAM is
just a collection of low-level authentication schemes which should provide a
general API to allow authentication without the developer knowing what exactly
is used in the background. According to the PAM manpage for Linux~\cite{pamman},
modules are split into four groups:

\begin{itemize}
\item The account group verifies if the user is permitted access and if the
password is not expired.
\item The authentication group verifies the user\textquotesingle s credentials.
These could be given in various ways, like a common password input, a
fingerprint, or as a combination of multiple ways.
\item The password groups is used to update and manage authentication
mechanisms. Modules from this group are usually connected with those from the
authentication group.
\item The session group provides ways to manage permissions for users after
authentication. It ensures the user stays authorised and also modules could
implement ways to log out users or check session termination from other sources.
\end{itemize}

Usually, Linux-based operating systems use the default \texttt{pam\_unix.so} for
all groups. This module looks up credentials from the \texttt{/etc/passwd} file.
Additionally, modules as \texttt{pam\_deny.so} and \texttt{pam\_permit.so} are
used to provide access right management for single services per default.

\section{Connecting Computers}

People are exchanging data between devices for several decades now. The Internet
started when scientists found a way to have universities share research results
between computers over the back then used Advanced Research Projects Agency
Network (ARPANET). Clark~\cite{arpanet} released a paper in 1988 about the
protocols used by the ARPANET and gives arguments on why stated protocols were
used. The network already used the TCP/IP protocol and showed requirements for
future interconnection networks. Furthermore, Clark~\cite{arpanet} also explains
the evolution of the Internet by stating the plans of connecting other networks
with the ARPANET.

With easier access to computers, more universities, companies and private
households wanted to be connected and exchange data. The Internet provided the
possibility to do this. The standard for Hyper-Text Transfer Protocol
(HTTP)~\cite{rfc7231} was introduced to provide files from servers to users. The
standard for network mail~\cite{rfc561} provides a uniform way to send messages
between users over the Internet. With the growing amount of users, security in
the network and the connected devices became more relevant. Therefore, protocols
to ensure security for the participants were introduced as standards.

\subsection{Webserver}

Web servers provide users with content on the Internet, mostly referred to as
the World Wide Web (WWW). There are many different software implementations for
such servers available, most of them being open source and free, such as
Apache~\cite{apacheweb} and \texttt{nginx}~\cite{nginxweb}, which are according
to Netcraft the two most used open source web servers~\cite{webserverpoll}.

Web servers use the Hyper-Text Transfer Protocol (HTTP)~\cite{rfc7231} to handle
requests and deliver the content to the client. Web servers usually deliver
websites encoded in Hypertext Markup Language (HTML) but can also deliver all
kinds of files including JavaScript (JS) and Content Style Sheet (CSS), which
are rendered together with HTML by web browsers on the client.

A client requests content from the server by sending a Uniform Resource Locator
(URL)~\cite{rfc3986}. The server then looks up the given path in the URL and
answers the request by sending the requesting file or an error, if one occurs
during the handling of the request. As not all requests and answers should be
publicly visible, web servers adopted TLS for security and privacy reasons to
provide content over HTTPS. The client, most of the time a web browser, can
thereby verify the server\textquotesingle s identity and apply encryption to the
connection if required.

\subsubsection{HTTP - Basic Access Authentication}

A standard for access control was added to HTTP, as described in RFC
7617~\cite{rfc7617}, to allow only authorised users to access specified paths on
web servers. This technique makes use of a header field in HTTP, namely
\texttt{WWW-Authenticate}. RFC 3986~\cite{rfc3986} deprecates the functionality
to transmit credentials inside the requesting URL, only the HTTP header may be
used for authentication. Basic Access Authentication is an access-control
technique which does not protect the transmitted credentials. Therefore, it
should be used together with HTTPS to protect the transmitted data. The client
has to transmit the credentials with every request, and there is no logout
function as in other techniques using cookies or session identifiers.

\subsubsection{Access control in Apache and \texttt{nginx}}

Apache makes use of so-called \texttt{ht} files, which are an extension to the
usual server configuration for users with limited
permissions~\cite{apacheauthdoc}. Apache introduced \texttt{htpasswd}-files to
make use of Basic Authentication, these files store user credentials. The
developers of \texttt{nginx} later adapted these files for their web server
implementation. The \texttt{htpasswd}-files keep the data either in plaintext or
hashed. To create these files Apache provides the tool
\texttt{htpasswd}~\cite{htpasswddoc}. This tool uses MD5 hashing per default (as
of version 2.2.18), but it is possible to change the hashing algorithm to
something more secure, like bcrypt by Provos~\etal~\cite{bcrypt}. On a request
including Basic Authentication, the web server checks the \texttt{htpasswd} file
to verify the given credentials and then returns the requested content or a
permission error according to RFC 7231~\cite{rfc7231}.

\subsection{Transport Layer Security}

Transport Layer Security (TLS) is a protocol proposed by the Internet
Engineering Task Force (IETF). The TLS protocol was released to provide secure
communication on the Internet. The current version of TLS, 1.2, as in RFC
5246~\cite{rfc5246}, will be obsolete with the upcoming release of TLS 1.3, as
described in RFC 8446~\cite{rfc8446}.

TLS is used in web servers to provide secure connections with their clients.
Also, applications like Virtual Private Networks (VPNs) and Secure Shell (SSH)
make use of protocols and cyphers described in the TLS standard.
OpenSSL~\cite{opensslweb} is an open source library providing the functionality
described by the standard.

TLS provides a secure connection between various devices, usually clients and a
server. The connection holds the following properties.

\subsubsection{Authenticated Identities}

Parties using TLS want to ensure that the other party is the one they claim to
be. To protect users from phishing or accessing wrong servers, TLS provides
certificates to authenticate identities. The client requests the certificate
from the server, which contains its public key and possible key exchange methods
it supports. The certificates may be signed by a certificate authority which the
client already trusts.

\subsubsection{Private Connections}

According to the TLS handshake protocol, described in RFC 5246~\cite{rfc5246}
Section 7. The server and the client agree on a cypher to encrypt data sent
between them. TLS 1.2 states that 128-bit AES needs to be supported, but parties
may agree on another cypher during the handshake. As AES uses a symmetric key to
encrypt data, this key has to be exchanged between the two parties. Therefore
the parties agree on a key exchange protocol during the handshake. TLS 1.2
states that RSA~\cite{rsapaper} needs to be supported, but parties can choose a
different method, such as Diffie-Hellman key exchange~\cite{deffhellpaper}, or
use a pre-shared key.

\subsubsection{Reliability}

The TLS standard describes message authentication codes (MAC) to allow parties
to detect alteration of sent messages. These codes are cryptographic hashes,
which use the data contained in a message plus some secret data. Methods to
protect message integrity in such a way are described in RFC 2104~\cite{rfc2104}

\subsection{Cryptographic Nonces}

In the early days of encrypted communication, replay-attacks were a common
problem, as Syverson showed in his taxonomy of replay attacks in
1994~\cite{replaytax}. Rogaway~\cite{nonceintro, nonceassoc}, published a
solution to this problem. In his work, he proposes to use one-time,
pseudo-random data to protect symmetric cryptography from replay-attacks. He
calls such data nonce. Rogeway~\cite{nonceassoc} states, that to ensure that
nonces will not be reused, they should be used as a counter with a random start
value.

However, even with this idea of replay-attack protection, current
protocols and technologies still face problems with nonces, like
Vanhoef~\etal~\cite{wpanoncereuse} showed in 2017, that it is possible to force
nonce reuse in WPA2.

\subsection{Advanced Encryption Standard (AES)}

The Advanced Encryption Standard is a block cypher proposed to the US National
Institute of Standards and Technology (NIST) by Rijmen and Daemen in
1999~\cite{aesproposal}. In 2001, NIST released AES as a specification for
encryption of electronic data. It is the successor of the Data Encryption
Standard (DES). AES is also part of ISO/IEC 18033-3~\cite{iso18033}, an ISO
standard related to security techniques and encryption algorithms.

As Rijmen and Daemen~\etal~\cite{aesproposal} proposed, AES is a block cypher
which operates on data blocks. The blocks are stored in a memory array,
represented as a matrix of bytes. The algorithm consists of four steps, which
are reused in multiple rounds, the steps operate on the memory matrix by
shifting entries or replacing them with using a substitution box (S-BOX), this
provides non-linearity to the algorithm. The algorithm works very similar for
encryption and decryption, where most steps can be reused, this makes it very
convenient to provide these functionalities directly in hardware. Intel
processors already provide instructions, which can directly be used for
AES~\cite{intelaes}.

\subsubsection{Attacks on AES}

Researchers and attackers targeted AES since it was proposed for cryptographic
usage. Most of these attacks targeted a smaller number of
rounds than the algorithm describes. In 2011, Bogdanov~\etal~\cite{bicliqueaes}
showed an attack against AES which targets all of the rounds. Their attack
weakens the security of AES slightly, where for 128-bit key recovery the
computational complexity drops to $2^{126.1}$ and for 256-bit keys to
$2^{254.4}$. As these are still very high complexities, the attack does not
affect real-world usages of AES.

Way more common than cryptoanalysis attacks on AES are attacks targeting
software and hardware implementations. For example,
O'Flynn~\etal~\cite{aespowerboot} showed an attack against an AES implementation
used to decrypt a firmware image during the bootup process. Whereas most attacks
target 128 bits, they showed one against 256. O'Flynn~\etal~\cite{aespowerboot}
use a correlation power analysis on the software run by the bootloader which
loads an AES-256-CBC encrypted image. Besides recovering the full key, also the
initialisation vector is recovered by the attack.

Lo~\etal~\cite{sboxpoweranal} published another hardware side-channel attack.
The attack targets the implementation of the S-box used in AES-128. Besides
using correlation power analysis, they also use differential power analysis.
They compare both methods and recover cypher keys from function calls accessing
the S-BOX.

Neve~\etal~\cite{sidecomplex}, show that increasing the key-size in AES from 128
to 256 bits does not increase the complexity of side-channel attacks by the same
factor. Neve~\etal~\cite{sidecomplex} show that for cache-based side-channel
attacks an upgrade from 128 bits to 256 only increases the complexity of an
attack by a factor of 6 to 7.

\subsection{AES-GCM}

Galois Counter Mode (GCM)~\cite{gcm} was proposed as a standard by
NIST~\cite{gcmnist} to combine counting nonces with block cyphers, such as AES.
This combination allows the creation of a stream cypher where each block is
encrypted with a different nonce. With the addition of GCM, also a MAC is
generated, in this algorithm called an authentication tag.
Figure~\ref{fig:aesgcm} shows the schematic of the proposed AES-GCM. For the
description of the algorithm, a similar notation is used as
Böck~\etal~\cite{gcmnonceattack} use in their paper about attacks against GCM.

\begin{itemize}
\begin{samepage}
  \item[$CNT_i$] The $i$-th counter block, computed using the concatenation of
the initialisation vector ($IV$), with a size of 96 bits, and the counter value
$cnt$, $cnt = (i+1) \mod{2^{32}}$, to gain a 128-bit value. $CNT_0 = IV
\parallel 0 ^{31} \parallel 1$.
  \item[$E_k$] AES encryption with symmetric key $k$.
  \item[$a \oplus b$] XOR operation between $a$ and $b$.
  \item[$P_i$] The $i$-th plaintext block.
  \item[$C_i$] The $i$-th ciphertext block.
  \item[$mult_H$] Multiplication $H \cdot X$ in Galois Field GF($2^{128}$),
with the irreducible polynomial $g = g(x) = x^{128} + x^{7} + x^{2} + x + 1$.
  \item[$A$] Authenticated data.
  \item[$len(X)$] Bit-length of $X$.
  \item[$a \parallel b$] Concatenation of $a$ and $b$.
  \item[$TAG$] Authentication tag.
\end{samepage}
\end{itemize}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \node (cnt0) [block]  {$CNT_0$};
    \node (cnt1) [block, right of=cnt0, xshift=3cm]  {$CNT_1$};
    \node (cnt2) [block, right of=cnt1, xshift=3cm]  {$CNT_2$};
    \node (enc0) [function, below of=cnt0, yshift=-0.5cm] {$E_k$};
    \node (enc1) [function, below of=cnt1, yshift=-0.5cm] {$E_k$};
    \node (enc2) [function, below of=cnt2, yshift=-0.5cm] {$E_k$};
    \node (xor11) [XOR, below of=enc1, scale=2, yshift=-0.5cm] {};
    \node (xor12) [XOR, below of=enc2, scale=2, yshift=-0.5cm] {};
    \node (pt1) [block, left of=xor11, xshift=-1cm] {$P_1$};
    \node (pt2) [block, left of=xor12, xshift=-1cm] {$P_2$};
    \node (ct1) [block, below of=xor11, yshift=-0.5cm] {$C_1$};
    \node (ct2) [block, below of=xor12, yshift=-0.5cm] {$C_2$};
    \node (xor21) [XOR, below of=ct1, scale=2, yshift=-0.25cm] {};
    \node (xor22) [XOR, below of=ct2, scale=2, yshift=-0.25cm] {};
    \node (mult1) [function, left of=xor21, xshift=-1.5cm] {$mult_H$};
    \node (mult2) [function, right of=xor21, xshift=1cm] {$mult_H$};
    \node (mult3) [function, below of=xor22, yshift=-0.5cm] {$mult_H$};
    \node (auth) [block, below of=mult1, yshift=-0.5cm] {$A$};
    \node (xor31) [XOR, below of=mult3, scale=2, yshift=-0.25cm] {};
    \node (len) [block, left of=xor31, xshift=-1cm] {$len(A) || len(C)$};
    \node (mult4) [function, below of=xor31, yshift=-0.5cm] {$mult_H$};
    \node (xor41) [XOR, below of=mult4, scale=2, yshift=-0.25cm] {};
    \node (tag) [block, right of=xor41, xshift=1cm] {$TAG$};

    \draw [arrow] (cnt0) -- (cnt1);
    \draw [arrow] (cnt1) -- (cnt2);
    \draw [arrow] (cnt0) -- (enc0);
    \draw [arrow] (cnt1) -- (enc1);
    \draw [arrow] (cnt2) -- (enc2);
    \draw [arrow] (enc0) |- (xor41);
    \draw [arrow] (enc1) -- (xor11);
    \draw [arrow] (enc2) -- (xor12);
    \draw [arrow] (pt1) -- (xor11);
    \draw [arrow] (pt2) -- (xor12);
    \draw [arrow] (xor11) -- (ct1);
    \draw [arrow] (xor12) -- (ct2);
    \draw [arrow] (ct1) -- (xor21);
    \draw [arrow] (ct2) -- (xor22);
    \draw [arrow] (mult1) -- (xor21);
    \draw [arrow] (auth) -- (mult1);
    \draw [arrow] (xor21) -- (mult2);
    \draw [arrow] (mult2) -- (xor22);
    \draw [arrow] (xor22) -- (mult3);
    \draw [arrow] (mult3) -- (xor31);
    \draw [arrow] (len) -- (xor31);
    \draw [arrow] (xor31) -- (mult4);
    \draw [arrow] (mult4) -- (xor41);
    \draw [arrow] (xor41) -- (tag);
  \end{tikzpicture}
  \caption{Schematic of AES-GCM. The $E_k$ blocks apply AES-encryption with the
key $k$. The $mult_H$ blocks apply a multiplication in
the Galois-field. $A$ represents a block of authenticated data, which is added
to the MAC $TAG$ but not encrypted.}
  \label{fig:aesgcm}
\end{figure}

The following steps describe how AES-GCM is used to encrypt data:

\begin{enumerate}
\begin{samepage}
  \item An initialisation vector $IV$ of 96 bits is generated.
  \item The counters $CNT_i$ of 128 bits are computed with $CNT_i = IV \parallel
cnt$, where $cnt = (i + 1) \bmod{2^{32}}$, for $i \in\{0 .. n\}$, $n$
represents the number of plaintext blocks.
  \item The ciphertexts $C_i$ are generated by encrypting the counter values
$CNT_i$ and XORing them to the plaintext $P_i$, $C_i = P_i \oplus E_k(CNT_i)$.
\end{samepage}
\end{enumerate}

To get the authentication tag $TAG$, the $GHASH$ function,
$GHASH(H, A, C) = X_{m+n+1}$, is applied. Where, $H$ is the hash key, computed
by encrypting 128 zero bits with the AES block cypher, $C$ is the ciphertext,
and $A$ is non-encrypted but authenticated data. Figure~\ref{fig:aesgcm}
shows how the $GHASH$ is computed for one block of authenticated data and two
blocks of plaintext. The following steps describe the computation of the $TAG$:

\begin{enumerate}
\begin{samepage}
  \item The Galois field multiplication is applied to the block of
authenticated data. $R_0 = mult_H(A_1)$.
  \item The result of this is XORed to the first ciphertext. The output
of this XOR is again multiplied in the Galois field. $R_1 = mult_H(R_0 \oplus
C_1)$.
  \item This output is then XORed to the second ciphertext and again
multiplied. $R_2 = mult_H(R_1 \oplus C_2)$
  \item Then the length of the authenticated data and the ciphertext is
added. $R_3 = mult_H(R_2 \oplus (len(A) \parallel len(C)))$
  \item The hash key is added in the end to form the tag $TAG$. $TAG = H \oplus
mult_H(R_3)$.
\end{samepage}
\end{enumerate}

The authenticated tag $TAG$ can therefore be found by solving the polynomial
$g(X) = A_{1}X^{m+n+1} + \cdots + A_{m}X^{n+2} + C_{1}X^{n+1} + \cdots +
C_{n}X^2 + LX + S$, as $g(H) = TAG$. $L$ is the combined length of $A$
and $C$, and $S$ is the nonce plus the counter. We refer to the NIST
standard about AES-GCM~\cite{gcmnist} for more details.

\subsubsection{OpenSSL source code}

The libraries souce code is structured in a way that each part of a
cypher is implemented on its own. For our thesis and analysis we want to
refer to version \texttt{1.1.0g} of OpenSSL~\cite{opensslsource}. The AES-GCM
code part is located at \texttt{crypto/evp/e\_aes.c}. The structures used are
defined in \texttt{crypto/evp/evp\_locl.h},
\texttt{crypto/include/internal/evp\_int.h} and \texttt{ssl/ssl\_locl.h}. The
file \texttt{crypto/evp/evp\_lib.c} shows the implementation for the creation of
the cypher data.

In the OpenSSL library, the $IV$ for AES-GCM is computed by using twelve
bytes, where four bytes are a salt computed during the TLS handshake, and
additional eight bytes represent the used nonce.

\subsubsection{AES-GCM Nonce Reuse Attack}

Joux~\cite{NISTGCMcomment} calls his attack against GCM a ``forbidden attack''
because in cryptography the uniqueness of nonces is seen as given.
Böck~\etal~\cite{gcmnonceattack} describe how relevant nonce reuse still is in
real-world application and why clear definitions for nonces in protocols are
required.

Böck~\etal~\cite{gcmnonceattack} present an attack based on the simplified
attack by Joux, which he presented in his comment on the NIST
proposal~\cite{NISTGCMcomment}.  In those attacks, it is assumed that the same
nonce is used for two messages. Each of those messages consists of a single
ciphertext block and no additional authenticated data. It is also noted, that
all values beside $S$, the secret nonce value, are known to the attacker.

\[g_1(X) = C{_{1}X^2 \oplus L_1X \oplus S}\]
\[g_2(X) = C{_{2}X^2 \oplus L_2X \oplus S}\]

We add the corresponding tag $TAG$ to each polynomial to get $g_i'(X)$ as
follows.

\[g_1'(X) = C{_{1}X^2 \oplus L_1X \oplus S \oplus TAG_1}\]
\[g_2'(X) = C{_{2}X^2 \oplus L_2X \oplus S \oplus TAG_2}\]

Knowing that $g_x'(H) = 0$, because $g_x(H) = T_x$, we combine those two
formulas to the following polynomial.

\[g_{1+2}'(X) = (C_{1} + C_{2})X^2 + (L_1 + L_2)X + TAG_1 + TAG_2\]

This polynomial should be $0$ for the point $H$, $g_{1+2}'(H) = 0$. Therefore,
an attacker can calculate a list of values for $H$ which solve this equation.
The number of possible solutions for the polynomial is reduced with an
increasing number of nonce reuses. Therefore, by using the same nonce more often
it gets easier for an attacker to find $H$ and therefore break the used
cryptography.

\section{Software-based Microarchitectural Attacks}

Computers can leak information by behaving differently during the computation of
different data. Aciicmez~\etal~\cite{branchpower, rsaanal, branchopenssl}
released work on timing leaks which leak information about used key or data.
Aciicmez~\etal~\cite{rsaanal} show how RSA implementations were attackable with
microarchitectural analysis. For the implementation in OpenSSL, it was possible
to detect called reduction steps in the Montgomery multiplication, which lead to
a total break of the RSA implementation. Aciicmez~\etal~\cite{branchopenssl},
also show further microarchitectural attacks against OpenSSL, which break other
functions implemented in the open source library.

But not only cryptography is affected by side-channel attacks.
Weinberg~\etal~\cite{weinberg2011still} showed, it is also possible to gather
information about a users browser history by using side-channels.

\subsection{Low-Level Performance Measuring}

Many side-channel attacks are based on timings. Like, predicting what a machine
currently does because of the time it takes to compute something. Researchers
and attackers using time measurement as an information source, therefore usually
try to find a way to measure time with a high resolution.

\subsubsection{Native Timing Measurement}

In native code, the highly precise timestamp counter (TSC), provided on x86
architectures since the first Pentium CPU~\cite{intelsys}, can be used for
timing measurement. The Intel manual~\cite{intelsys} Section 4, states that the
current timestamp is stored in a model-specific 64-bit register (MSR). It also
states that the value in the MSR is increased with every clock cycle and reset
to zero on every processor reset.

Calling the instruction \texttt{RDTSC} will load the current value of the MSR
into the \texttt{EDX:EAX} registers. The \texttt{EDX} register is loaded with
the higher-order 32 bits, and the \texttt{EAX} register with the lower-order 32
bits. On architectures using Intel 64 the higher-order 32 bits of \texttt{RAX}
and \texttt{RDX} are cleared. \texttt{RDTSC} is not a serialising instruction,
that means the counter MSR could be read before the previous instruction is
finished. This behaviour is often not wanted, as results may not be accurate
enough. In the Intel manual~\cite{intelsys} it is therefore recommended to use
either the serialising \texttt{RDTSCP} instruction or use the sequence
\texttt{LFENCE;RDTSC}, if it is required for \texttt{RDTSC} to be called after
previous instructions finished.

\subsubsection{JavaScript Timing Measurement}

Calls to the \texttt{RDTSC} instruction are generally only possible in native
code. Van Goethem~\etal~\cite{webtime} showed how modern web browsers allow new
side-channels by providing accurate timing measurements. They show how the
JavaScript timestamp function \texttt{performance.now()} can be used for timing
measurements. Mozilla reacted to the released timing attack by lowering the
resolution of the timing API to five microseconds~\cite{moztime}.
Schwarz~\etal~\cite{fantastictimers} show how this countermeasure can be
bypassed and give an evaluation of new sources of timing information in
JavaScript. Lipp~\etal~\cite{armageddon, keytimejs} show how a counting thread
can be used as an accurate timing source in JavaScript and also state that the
time measurement is accurate enough to detect keystroke interrupts.
Schwarz~\etal~\cite{fantastictimers} state that with this accuracy of timings it
is possible to perform cache-attacks and Rowhammer inside web browsers.

\subsection{Caches}

Vendors of processors introduced the principle of caching to memory management
to speed up access times for memory. According to the Intel architecture white
paper~\cite{intelwhite}, modern CPUs implement three or four levels of caching.
As shown in Figure~\ref{fig:intelcache}, the design in Intel Kaby Lake and Intel
Haswell assigns caches for level one and two (L1, L2) to each core and the cores
share the level three cache (L3) between each other. L1 caches can be split up
in a data and an instruction cache. There might also be an L0 cache for
micro-operations, as it is also implemented in the two described architectures.

\begin{figure}[!htb]
  \centering
  \begin{tikzpicture}
    % Level 1 caching blocks
    \node (C1L1) [block] {$C_1-L_1$};
    \node (C2L1) [block, right of=C1L1, xshift=1.5cm] {$C_2-L_1$};
    \node (C3L1) [block, right of=C2L1, xshift=1.5cm] {$C_3-L_1$};
    \node (C4L1) [block, right of=C3L1, xshift=1.5cm] {$C_4-L_1$};
    % Level 2 caching blocks
    \node (C1L2) [block, below of=C1L1, yshift=-1cm] {$C_1-L_2$};
    \node (C2L2) [block, right of=C1L2, xshift=1.5cm] {$C_2-L_2$};
    \node (C3L2) [block, right of=C2L2, xshift=1.5cm] {$C_3-L_2$};
    \node (C4L2) [block, right of=C3L2, xshift=1.5cm] {$C_4-L_2$};
    % Level 3 block
    \node (L3) [level3block, below of=C2L2, yshift=-1cm, xshift=1.25cm] {$L_3$};
    % Connections
    \draw [doublearrow] (C1L1) -- (C1L2);
    \draw [doublearrow] (C2L1) -- (C2L2);
    \draw [doublearrow] (C3L1) -- (C3L2);
    \draw [doublearrow] (C4L1) -- (C4L2);
    \draw [doublearrow] (C1L2) -- (0,-3.6cm);
    \draw [doublearrow] (C2L2) -- (2.5cm,-3.6cm);
    \draw [doublearrow] (C3L2) -- (5cm,-3.6cm);
    \draw [doublearrow] (C4L2) -- (7.5cm,-3.6cm);
    % Memory Controler and DDR3
    \node (MC) [mcblock, below of=L3, yshift=-1cm] {Memory Controller};
    \node (DDR31) [block, below of=MC, yshift=0.2cm, xshift=-1cm] {DRAM};
    \node (DDR32) [block, below of=MC, yshift=0.2cm, xshift=1cm] {DRAM};
    \draw [doublearrow] (L3) -- (MC);
  \end{tikzpicture}
  \caption{Schematic of Intel Kaby Lake caching architecture. Whereas $C_x$
represents each processing core and $L_x$ the level of the cache. Requests are
sent from the processing core to each memory storage, the one finding the
desired memory first will answer before the others. Caches get updated with
each request.} \label{fig:intelcache}
\end{figure}

Caches are updated on each memory access. Inclusive caching is a form of
managing the caches in a way where data stored on one level of caching is also
placed in lower cache levels. The microarchitecture defines if the CPU uses
inclusive caches and which ones are inclusive to others. The memory management
part of the processor needs to validate memory all the time to make sure memory
accesses do not get wrong data from other cache levels or the DRAM. Computation
at rates of about four gigahertz would not be possible without such caching
mechanisms.

We refer to the Intel manual~\cite{intelsys} chapter 11.1 and the Intel white
paper about the architecture~\cite{intelwhite} for more details about cache
management in modern CPUs.

\subsubsection{Cache Attacks}

Cache Attacks are part of microarchitectural attacks, where scientists have
shown that caching also introduces security risks to systems.
Gruss~\etal~\cite{gruss2015cache} showed how caches provide information by
measuring memory access-timings. They also showed how to automate attacks on
last-level caches. Knowledge gained from this kind of attack has also played
a role in the exploitation of the Rowhammer bug or other hardware bugs, such as
Meltdown~\cite{meltdown} and Spectre~\cite{spectre}.

\subsubsection{FLUSH+RELOAD}

This cache attack is a low noise, L3 cache side-channel attack, released
by Yarom~\etal~\cite{flushreload}. The attack provides a possibility of leaking
information about memory accesses on shared pages between two processes. As the
attack targets the last-level cache, it does not require the two processes to
run on the same core. Pages are either shared when actively mapped with such
property or mapped as shared with copy-on-write when a process is forked. One
process is referred to as spy and the other as victim. The attack consists of
three phases.

\begin{enumerate}
\begin{samepage}
\item The spy uses the instruction \texttt{clflush} to remove the given address
from the CPU caches.
\item The spy waits a pre-defined time.
\item The spy accesses the flushed memory with reading access. If it takes less
than a defined time, the victim has accessed the memory in this timespan.
\end{samepage}
\end{enumerate}

With this attack, certain information can be leaked to the spy processes. For
example, in cryptographic algorithms, accessed code lines could leak the private
key to the attacker.

\subsubsection{FLUSH+FLUSH}

This is a cache attack designed by Gruss~\etal~\cite{flushflush} to circumvent
detection of cache attacks such as FLUSH+RELOAD. This attack only uses only the
\texttt{clflush} instruction and no memory accesses. With this idea, the attack
bypasses cache attack detection by performance counters. Instead of the three
phases in FLUSH+RELOAD, this attack only has one phase. The spy process gains a
memory address to target, will call \texttt{clflush} on it and measure the time
it took to flush the cache. There are two possible outcomes for this instruction
call:

\begin{itemize}
\begin{samepage}
\item The CPU has not loaded the memory into the cache and the \texttt{clflush}
results in a cache-miss. Therefore the instruction can return immediately and
not flush any other cache levels.
\item The victim process has caused the CPU to load the memory into the cache.
Therefore the flush needs to be applied to all cache levels, which results in
longer execution time for the \texttt{clflush} instruction.
\end{samepage}
\end{itemize}

With measuring the execution time of \texttt{clflush}, the spy process can again
tell when a victim has accessed targeted memory, as the timing will be higher in
such cases. Besides the leakages similar to the FLUSH+RELOAD results,
Gruss~\etal also built a side-channel with this principle to transmit
information between two processes, according to their paper~\cite{flushflush},
they reach a transmission speed of $496$ kilobytes per second.

\subsection{Impact of Microarchitectural Attacks}

With growing knowledge on the CPU and caching level, researchers found major
security vulnerabilities in modern CPUs. Attacks like Meltdown, Spectre and
Foreshadow were reported by many newspapers and influenced the awareness of
microarchitectural attacks by the general public.

\subsubsection{Meltdown}

Lipp~\etal~\cite{meltdown} presented an attack targeting the memory isolation
between the operating system\textquotesingle s kernel and the userland memory
and shows how the CPU\textquotesingle s out-of-order execution can lead to a
leak of information from kernel memory to a user program. Meltdown makes use of
a side-effect of this execution method. The CPU wants to use as many computing
units as possible. It will execute code as soon as all dependencies are
resolved. So inside a program computing could happen ahead of other code parts.
In a simplified view, the attacker accesses kernel memory, which the CPU reads
out-of-order, the CPU executes other code and receives an exception because of
the disallowed memory access, which already happened before. This execution
order still makes accesses to caches and DRAM as it would normally access the
memory. Therefore the cache could be used to transmit the information. Meltdown
makes use of the Flush+Reload cache attack, as they use the read kernel memory
to flush certain cache lines. Access times to memory then tells which cache
lines the program cleaned. Hence it leaks the kernel memory content. Lipp~\etal
propose the KAISER countermeasure as a fix for this vulnerability. This work by
Gruss~\etal~\cite{kaiserpaper} is used to separate kernel and user memory
addresses. Their proof-of-concept implements a complete separation of the two
memory spaces inside the Linux kernel, which makes sure no information about the
kernel execution is leaked to the userspace.

\subsubsection{Spectre}

Kocher~\etal~\cite{spectre} presented this attack also targeting information
leakage by features modern CPUs provide. The attack gained its name form
exploiting speculative execution which makes use of the branch-prediction unit
inside the CPU. The CPU executes code which maybe needs execution later on,
the CPU executes multiple branches and results are either thrown away or
reported to the program later on. Spectre uses this behaviour to leak
information to an attacker by using side-channels inside the predicted code.
The attack breaks multiple security measures which would usually be in place in
modern operating systems. In their work, the researchers show multiple variants
of exploiting the different behaviours caused by speculative execution. For
example see the code in Listing~\ref{lst:spectre1}, as shown in their work. The
speculative execution executes the array access before the if condition check.
The \texttt{array1[x]} can point to kernel memory, for example, returning some
secret, then the \texttt{array2} is accessed at a page which refers to the
secret. This can later be checked again using cache accesses timings using
Prime+Probe or Flush+Reload again. The attacker can see the outcome of the
accesses after the condition check, even if it fails, as it was executed
speculatively. Other variants make use of similar behaviours of the CPU. The
researchers also show how the Spectre attack can be applied using JavaScript.

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=CStyle,
                   caption={Code for Spectre variant one, showing a
conditional branch example, as seen by the work done by
Kocher~\etal~\cite{spectre}.},
                   label={lst:spectre1}]
if(x < array1_size)
  y = array2[array1[x] * 4096];
\end{lstlisting}
\end{minipage}

\subsubsection{Foreshadow}

An attack by van Bulck~\etal~\cite{foreshadow}, targeting the Guard eXtension
(SGX) in current Intel CPUs. The researchers use already known speculative
execution bugs to gain knowledge of secret memory used inside the SGX
environment which is transferred to the CPU\textquotesingle s cache. The attack
works similar to the Meltdown attack, but it is more challenging because of the
SGX and its additional memory protection layers. With this attack, the security
promises on the hardware layer made by Intel are broken. Countermeasures
against this attack cannot be applied to the kernel layer. Intel needs to patch
their microcode in order to prevent the Foreshadow attack.

\subsection{Rowhammer}

Technology is steadily changing and improving, with vendors of computer parts
being forced to produce cheaper and better hardware every year. For DRAM this
resulted in less quality management and a declining number of proper checks for
hardware faults. However, the industry produced smaller and faster memory chips.
These new chips need less space and energy to store electric load, which
represents data. Years after vendors introduced increased densities in DRAM to
the market, researchers like Kim~\etal~\cite{rowhammergeneral} found a critical
bug in the chip design. With the little space and energy stored it was possible
to use changes of charges in memory rows to interfere with data of nearby cells.
This interference resulted in a change of bits in other memory rows.

This behaviour and bug then caused researchers to dig deeper into the
possibilities, and several attacks were found based on the Rowhammer bug. For
example, Google\textquotesingle s ``Project Zero''~\cite{projectzerorow} showed,
it is possible to use Rowhammer for privilege escalation and sandbox escapes.
Van der Veen~\etal~\cite{drammer} state that this attack not only affects
desktop computers but also memory in mobile devices. In their work,
Gruss~\etal~\cite{flipinthewall} show that flipping bits in memory has further
risks than previously thought and that Rowhammer defences need a general
overhaul. They state that the only solution is to fix the hardware.
Gruss~\etal~\cite{rowhammerjs} also showed that it is possible to trigger
bitflips from JavaScript.

\subsubsection{Design of Dynamic Random-Access Memory (DRAM)}

Random-access memory is designed to store information as bits. A simplified view
of RAM is a circuit containing amplifiers, one-bit storage cell matrix, an
address decoder and buffers for input and output of the memory.
Figure~\ref{fig:DRAMscheme} shows a simple schematic of a DRAM module. The
storage matrix is a two-dimensional array of one-bit storage cells. Accesses to
DRAM do not happen per bit but per row. The size of a row depends on the DRAM
design, but it is usually a multiple of the word size. In modern systems, rows
are larger than 64 kilobytes. The CPU sends requests for memory to the
controller which then translates it to a sequence, where first the column
entries to read are set via the bit lines and then the desired row via the word
lines. DRAM chips contain many memory arrays and controllers which a data bus
connects with the processor.

A one-transistor (1-T) memory cell, as seen in Figure~\ref{fig:1Tstorage}, is
used to store a single bit. To access memory, first, the desired bit lines are
charged with $\frac{V_{CC}}{2}$, then the word line is also charged to switch on
the transistors. An amplifier is then used to detect changes of charge on the
bit lines. A discharged capacitor would cause the voltage on the bit line to
drop, whereas a charged capacitor would raise the voltage to its load. If a
charged or discharged capacitor represents true or false for the bit state,
depends on the design of the DRAM chip. Writing the bits works in a similar way,
where the bit lines of a row are either set to $V_{CC}$ or $0$, and then the
word line is used to switch on the capacitors on the row, making the capacitors
change their charge to the level of the connected bit line.

The capacitor used for the 1-T cell will either lose load over time or on
reading access. Therefore, these capacitors need to be periodically refreshed.
The JEDEC standard~\cite{jedec} sets the refresh cycle to a 64-millisecond
interval. For a usual chip with $8192$ rows, this means a refresh happens all
$7,8$ microseconds. The easiest way to do this is by reading a bit and writing
the response back to it. In the DRAM design, it is possible to refresh each row
at a time with a RAS without the following CAS. With this refresh cycle, the
chip needs to hold the information which was the last refreshed row. Some DRAM
designs, therefore, implement a so-called row refresh pointer holding this
information.

\begin{figure}
  \centering
  \begin{circuitikz}
  \ctikzset{label/align = smart}
  \draw
  (0,0) to[short, l_=$bit\ line$] (0,-4)
  (-1,-1) to[short, l^=$word\ line$] (4,-1)
  (1,-2) node[nmos, rotate=-90] (mos) {}
  (mos.gate) node[anchor=north] {}
  (mos.drain) node[anchor=east] {}
  (mos.source) node[anchor=west] {}
  ;
  \draw
  (mos.gate) to[short] (1,-1)
  (mos.source) to[short] (0,-2)
  ;
  \draw
  (mos.drain) to[short] (2,-2)
  (2,-3) node[rground] (A) {}
  node[anchor=west, yshift=-0.4cm, xshift=0.2cm] {} to[pC, l_=$C$] (2,-2)
  ;
  \end{circuitikz}
  \caption{Schematic of a 1-T memory storage element. The capacitor $C$ is used
to store information. The transistor makes it possible to access the storage by
applying charge on the connected lines. Depending on the voltage used it is a
read or a write access.}
  \label{fig:1Tstorage}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture}
  %blocks
  \node (arr) [memarray]  {Memory Array};
  \node (rowdec) [memfunc, left of=arr, rotate=90, yshift=1.5cm] {Row decoder};
  \node (amp) [memfunc, below of=arr, yshift=-1.5cm] {Amplifier};
  \node (coldec) [memfunc, below of=amp, yshift=-0.5cm] {Column decoder};
  \node (memcon) [memcon, left of=amp, align=center, xshift=-2cm,
                  yshift=-0.5cm] {Memory \\ Controller};
  \node (buf) [memfunc, right of=arr, rotate=90, yshift=-1.5cm] {Data Buffer};
  %connections
  \draw [arrow] (amp) -| (buf);
  % memcon -> rowdec
  \draw [arrow] (-2.5,-2) -- (-2.5,-1.5);
  % memcon -> coldec
  \draw [arrow] (-2,-3.75) -- (-1.5,-3.75);
  % coldec -> amp
  \draw [arrow] (-1,-3.5) -- (-1,-3);
  \draw [arrow] (1,-3.5) -- (1,-3);
  % amp -> arr
  \draw [arrow] (-1,-2) -- (-1,-1.5);
  \draw [arrow] (1,-2) -- (1,-1.5);
  \node at (0,-1.75) {bit lines};
  % rowdec -> arr
  \draw [arrow] (-2,-1) -- (-1.5,-1);
  \draw [arrow] (-2,1) -- (-1.5,1);
  \node at (-1.75,0) [rotate=90] {word lines};
  % arr <-> buf
  \draw [doublearrow] (1.5,-1) -- (2,-1);
  \draw [doublearrow] (1.5,1) -- (2,1);
  % memcon <-> buf
  \draw [arrow] (-3,-5) -- (memcon.south);
  \draw [line] (-3,-5) -- (3.5,-5);
  \draw [line] (3.5,-5) -- (3.5,0);
  \draw [arrow] (3.5,0) -- (3,0);
  \end{tikzpicture}
  \caption{Schematic of a single DRAM module. The memory controller is managing
the data buffer to fill or read the buffer via a bus connection to the CPU. The
decoders are used to make sure the correct bits are accessed. The amplifier is
used because the bit voltage differences are usually too small to change the
memory in the buffer directly.}
  \label{fig:DRAMscheme}
\end{figure}

\subsubsection{Introducing bitflips to DRAM}

Kim~\etal~\cite{rowhammergeneral} show how it is possible to cause bits to flip
inside memory by correct access patterns. They use two rows as so-called
aggressors, and one as target row. As the code in
Listing~\ref{lst:rowhammercode} shows, the two aggressor rows are repeatedly
read, causing memory accesses in the DRAM. The changes in electrical charge
caused by the read accesses will by chance interfere with the loads in the
target row and change content there. The \texttt{clflush} instructions are used
to empty the memory in the used cache lines, causing the System to access the
DRAM for each read instead of getting the value out of the cache. These accesses
happen fast enough so that the refresh cycle of the DRAM cannot ensure that the
capacitors hold enough voltage.

\begin{minipage}{\linewidth}
\begin{lstlisting}[
  label={lst:rowhammercode},
  style=nasm,
  caption={Code to trigger the Rowhammer bug, as of
Kim~\etal~\cite{rowhammergeneral}. The preating accesses to the aggressor
locations cause bitflips in a third victim row. The \texttt{CFLUSH} is
neededed to make sure the DRAM rows are accessed directly. According to
Project Zero~\cite{projectzerorow} the \texttt{mfence} is not needed and even
lowered the number of flips.},]
code1a:
  mov (X), %eax
  mov (Y), %ebx
  clflush (X)
  clflush (Y)
  mfence
  jmp code1a
\end{lstlisting}
\end{minipage}

The memory addresses to be accessed to gain the rows next to each other are hard
to find. Linux provides a file, \texttt{/proc/<PID>/pagemap}, where physical
positioning is stored. Some systems allow huge pages, where two megabytes of
memory are used contiguously. These two megabytes will use more than one row,
other than normal pages with the size of four kilobytes. In the paper,
Kim~\etal~\cite{rowhammergeneral} use addresses calculated based on their
knowledge of physical address mapping used by common CPU vendors. Project
Zero~\cite{projectzerorow} show how it is possible to use Rowhammer to
gain privilege escalation on a standard \texttt{x86\_64} CPU and an escape from
a sandbox. After vendors of operating systems started to introduce
countermeasures, scientists like Gruss~\etal~\cite{rowhammerjs, flipinthewall},
came up with further, improved attacks making use of the Rowhammer bug. They
show more possible targets for flips and also show that Rowhammer is possible in
JavaScript.
%}}}

%% vim:foldmethod=expr
%% vim:fde=getline(v\:lnum)=~'^%%%%\ .\\+'?'>1'\:'='
%%% Local Variables:
%%% mode: latex
%%% mode: auto-fill
%%% mode: flyspell
%%% eval: (ispell-change-dictionary "en_US")
%%% TeX-master: "main"
%%% End:
