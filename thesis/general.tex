%%%%%% General %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%{{{
\chapter{Background}\label{sec:general}

In this chapter, we describe the basics, techniques, tools, and programs used in
this thesis. It provides a general overview that is required to understand the
details of the work in the following chapters. We start by showing how modern
computers execute programs and code. We discuss file formats required to load
programs and how common computing architectures structure their machine code. We
then look at how programs and source code is tested and what new technologies
evolved in that matter. As we deal with Linux-based operating systems, we take a
look at the permission system used by these.

Additionally, we discuss topics in secure communication. We describe the basic
principles of network connections, web servers, and their security. As the
thesis provides automation for searching for bits to flip with Rowhammer, we
also look at this kind of attack, how it works, and discuss microarchitectural
attacks in general.

\section{Executing Programs}

Processing units (PU) execute machine code. Any device running programs contains
at least one PU. Usually, the central processing unit (CPU) runs programs and
triggers other PUs to run tasks if needed. Often referred to as the main
processor, the CPU carries out the instruction given to it by the machine code
representing a program. A PU can only step over single instructions and execute
them one by one. This is done by moving the instruction pointer to different
memory locations. Simple processors may only execute one single program. On
ordinary desktop computers, the operating system is the main program which can
load other programs as processes and manage them. CPUs can only execute machine
code, but programs are usually written in human-readable programming languages
which are then translated to machine language by either a compiler or an
interpreter. We refer to files created by a compiler as executables or binaries.
These files can directly be loaded into memory by the operating system.
Operating Systems use defined structures for executables to make this loading
possible, for example, GNU/Linux systems use the executable and linkable format
(ELF), and Windows uses the portable executable format (PE).

\subsection{CPU Instructions}

Machine code, in general, is a list of instructions, encoded in some binary
format. Usually, an instruction consists of an operation code (opcode), telling
the PU what to do, and parameters for the opcode, which the processor
uses for the operation. Instructions can operate on registers or locations in
memory directly. Available instructions and their design are depending on the
architecture used. The most common instruction set architectures (ISA) are Intel
$x86$, mostly referred to as \texttt{x86\_64} or \texttt{AMD64}, and ARM ISA
which is used by most mobile processors.

\begin{table}[]
\centering
\begin{tabular}{ccccccc}
\cline{2-7}
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Prefix} &
\multicolumn{1}{c|}{Opcode} & \multicolumn{1}{c|}{ModR/M} &
\multicolumn{1}{c|}{SIB} & \multicolumn{1}{c|}{Displacement} &
\multicolumn{1}{c|}{Immediate} \\ \cline{2-7}
bytes         & 1 per prefix                & 1, 2 or 3
 & 0 or 1                      & 0 or 1                   & 0, 1, 2 or 4
             & 0, 1, 2 or 4
\end{tabular}
\caption{Instruction Format for Intel 64 and IA-32
architectures~\cite{intelsys}, showing how many different instruction lengths
are possible in modern architectures and also which parts they may contain.}
\label{tab:instrfor}
\end{table}

Table~\ref{tab:instrfor} shows the instruction format for Intel 64 and IA-32
architectures. The format splits an instruction into the following sections:

\begin{itemize}
  \item Prefix - Used to give the CPU further information on how to handle the
opcode. There are prefixes for memory locks in a multi-core environment or
repeat instructions to apply an operation on each byte in a string or I/O data.
There are also prefixes overriding the operand sizes or introducing SSE
instruction calls, and even others give branching hints to the CPU.
  \item Opcode - Tells the CPU what to do and how to handle the following
parameters.
  \item ModR/M - This byte is used behind the opcode to tell the CPU what
addressing mode to use for memory accesses.
  \item SIB - This byte also needs to be parsed and evaluated for some ModR/M
addressing-modes.
  \item Displacement - Some addressing-modes set by the ModR/M or SIB byte
might include a displacement following them.
  \item Immediate - Immediate data might follow if required by previous
parameters.
\end{itemize}

We refer to the Intel Manual~\cite{intelsys} section 2.1 for further details on
the instruction format used in \texttt{x86\_64}.

\subsection{Executable and Linkable Format (ELF)}

ELF is an object file format. It is used to describe a program and gives
instructions on how to load it into memory to make it ready for execution
by the processor without applying changes to the binary-content itself. A
linker creates ELF files after an assembler turned the program\textquotesingle s
source code into machine code. Most modern Linux-based operating systems follow
the ELF specification released by the Tool Interface Standard
Committee~\cite{elfspec}. Therefore, we need to understand the layout of such
executable files before introducing bitflips in them.

\subsubsection{Structure of an ELF file}

\begin{figure}
  \centering
  \begin{tikzpicture}
  \node (eh) [block] {ELF Header};
  \node (ph) [block, right of=eh, xshift=3cm] {Program Header Table};
  \node (sh) [block, below of=eh, yshift=-2cm] {Section Header Table};
  \node (dot0) [dot, below of=ph] {};
  \node (dot1) [dot, below of=dot0, yshift=0.5cm] {};
  \node (text) [block, below of=dot1] {.text};
  \node (rodata) [block, below of=text] {.rodata};
  \node (data) [block, below of=rodata] {.data};
  \node (bss) [block, below of=data] {.bss};
  \node (dot2) [dot, below of=bss] {};
  \node (dot3) [dot, below of=dot2, yshift=0.5cm] {};
  \node (seg02) [segblock, right of=text, xshift=1cm, yshift=-0.5cm] {02};
  \node (dot4) [dot, above of=seg02] {};
  \node (dot5) [dot, above of=dot4, yshift=-0.5cm] {};
  \node (seg03) [segblock, right of=data, xshift=1cm, yshift=-0.5cm] {03};
  \node (dot6) [dot, below of=seg03] {};
  \node (dot7) [dot, below of=dot6, yshift=0.5cm] {};
  % connections
  \path [thick,->,>=stealth] (eh) edge[out=0, in=180] (ph);
  \path [thick,->,>=stealth] (eh) edge[out=180, in=180] (sh);
  \path [thick,->,>=stealth] (sh) edge[out=0, in=180] (text);
  \path [thick,->,>=stealth] (sh) edge[out=0, in=180] (rodata);
  \path [thick,->,>=stealth] (sh) edge[out=0, in=180] (data);
  \path [thick,->,>=stealth] (sh) edge[out=0, in=180] (bss);
  \path [thick,->,>=stealth] (ph) edge[out=0, in=0] (seg02);
  \path [thick,->,>=stealth] (ph) edge[out=0, in=0] (seg03);
  \path [thick,->,>=stealth] (seg02) edge[out=180, in=0] (text);
  \path [thick,->,>=stealth] (seg02) edge[out=180, in=0] (rodata);
  \path [thick,->,>=stealth] (seg03) edge[out=180, in=0] (data);
  \path [thick,->,>=stealth] (seg03) edge[out=180, in=0] (bss);
  \end{tikzpicture}
  \caption{Schematics of the structure of an ELF file showing the four main
sections inside an executable. Sections and segments are connected via the
program header table and the section header table. Segments usually are just
numbers and can refer to multiple sections inside the section header table.}
\label{fig:elfstruct}
\end{figure}

There are two views of an ELF file, the linking view and the execution view.
Those are also called section and segment view respectively. They share the same
ELF header but serve different purposes for the operating system.
Figure~\ref{fig:elfstruct} show the connection between sections and segments via
the two different headers. Sections and segments can be seen as follows:

\subsubsection{Sections}

Sections describe the binary for the linking view. A section can contain
instructions, data, symbol table and relocation information. Sections reserved
for the system start with a dot. There can be additional sections defined by
the user. Sections are created and managed by the linker. Sections that are
directly loaded into the program\textquotesingle s memory image are initialized
data (\texttt{.data, .data1}), read-only data (\texttt{.rodata, .rodata1}), and
executable instructions (\texttt{.text}).

\subsubsection{Segments}

They describe the virtual memory layout of a loaded binary.
Figure~\ref{fig:elfstruct} shows how segments are used in an ELF file.
Usually, the linker splits segments into different behaviours the loader has to
apply. For example, all read-only data sections are in the same segment. When
loading the image into memory, references inside the ELF file need to be
resolved and loaded into the memory too. After successfully loading the image
and its dependencies the program can be executed.

\subsubsection{Loading ELF files into Memory}

As we focus on GNU/Linux operating systems, we will look at how UNIX System-V
Release 4-based operating systems handle ELF files in order to create running
programs. These systems do not use physical addresses during execution, and
the OS is free to change the position of sections in the virtual address space.
Therefore, in an ELF file, a section only contains a base address and offsets
to that address. For certain compilation settings the the operating system is
free to change the base address in the programs virtual address space. The
loader then places additional memory according to the offsets.

\subsubsection{Dynamic Linking}

In modern systems it is common to have multiple functions used by different
programs. Shared libraries can be used to provide such general functions. When
deploying programs, developers usually make sure all required shared libraries
exist on the target platform or ship their software with those included. When
depending on such system libraries, the loader will insert the shared-library
functions into memory at the desired entry point. This technique is called
dynamic linking. The loader fetches the information about the required library
from the ELF file and then checks the library paths provided by the operating
system if the library can to be loaded.

\section{Analysis and Testing of Executables}

Testing is a significant part of software development. There are a lot of
different approaches and styles for testing. One approach to test a program is
to provide input to it and check if the code operates as expected by validating
the output. With increasing amount of code and an increasing number of bugs
found, the style of testing changed over the years. On the one hand, developers
nowadays ideally provide unit tests inside their code to test single functions
and make sure they work as intended. On the other hand, sometimes the source
code is not available to a tester, or it is way too much work to read the entire
source code to create such tests. Therefore, developers came up with tools
which can be used to test binaries on their own without knowing the entire
source code.

\subsection{Fuzzing}

Black-box testing is an approach where a program is tested without knowing
its source code. Fuzzing is taking this to another level, where even knowledge
about the program itself is just minimal. The idea of applying just random
inputs to programs to test its robustness already came up in the 80s as
Duran~\etal~\cite{randtest} showed. Tools applying similar techniques, like
applying automatically-generated, often randomised, inputs are called fuzzers.
They try to reach corner cases in programs without knowing which exist and
detect undefined behaviour or crashes. Generally, fuzzers give an overall view
on the robustness of a program. They are cheap to apply and even can find bugs
usual white-box tests would miss.

If a fuzzer would only apply random inputs, testing would take a very long
time, as a lot of unneeded tests would run. Because of this, the technology
improved over time and different fuzzers evolved. For simple programs, just
testing common input mistakes, like negative numbers, newlines, end of file
characters, format strings, or just very long strings might be enough. Advanced
software requires better ways of fuzzing, for example, network protocols, file
systems, or image formats have a very complex structure which makes it hard to
find possible crashes by random data input. Fuzzers for such kind of software
generate inputs from given examples and apply small changes to them.
Xmlfuzzer~\cite{xmlfuzzer} is a tool crafted specially to test XML schemes in
programs. Besides special format fuzzers there are more advanced ones which
directly interact with the target, allowing to track execution paths, and
therefore, create even more test cases automatically. The most popular fuzzer is
``american fuzzy lop'' (afl)~\cite{aflweb}. It contains described algorithms to
get the best possible code coverage. Another more advanced fuzzer is Steelix by
Yuekang~\etal~\cite{steelix}, which provides program-state-based binary fuzzing.
Steelix uses static analysis and binary instrumentation to provide more
information about the fuzzing process and therefore gains better test cases and
code coverage.

Shoshitaishvili~\etal showed how fuzzing could find authentication bypasses in
binary firmware with their tool Firmalice~\cite{firmalice}.
Wang~\etal~\cite{inmemfuzzing} showed how in-memory fuzzing can be used to detect
similarities in binary code. This is done by fuzzing every function available
and comparing traces of program behaviours with machine learning models. They
show that they can find similarities in binaries even when different compilers,
and optimisations are used or when obfuscation is applied.

\subsection{Symbolic Execution}

Fuzzing provides a pseudo-random input to a program to test it. Therefore, it
might not catch all possible execution paths of a program. Techniques to test
all possible paths by simplifying the program, one of these techniques is
symbolic execution. The idea was already in place in 1976, when James C. King
released his work~\cite{symbolexechist} about symbolic execution. It makes use
of the possible simplification of inputs where they use a logic symbol with
fewer possible states instead. For example, a number can be represented by the
possible states: maximal value, minimum value, positive, negative, zero. The
possible symbolic-state inputs then run through the program in combination with
a constraint solver to report possible violations of the program\textquotesingle
s specification. The solver can also be used to find possible paths to reach a
pre-defined goal inside the binary.

One of the most used symbolic analysis frameworks is
\texttt{angr}~\cite{angrpaper}. Besides the symbolic execution of binaries, it
also allows various other analysis methods such as control-flow graph recovery,
automatic exploit generation or automatic binary hardening.

Stephens~\etal~\cite{driller} showed with Driller how symbolic execution could
be beneficial to fuzzing approaches. This allows keeping the number of execution
paths to test lower but also try to cover as much code as possible. Driller uses
concolic execution for the path exploring. Concolic execution is the combination
of symbolic and concrete execution of a program.

\subsection{Instrumentation}

Instrumentation comes down to adding new code to already existing one in an
automated manner. Developers use instrumentation as a tool for debugging or
performance measures by adding calls to timing functions or information logging
to the code. Information from instrumentation could be something like what
functions are called and how often, what execution branches are taken, how long
does a subroutine take, what memory is accessed and many more. It usually also
allows the developer to change the runtime environment of a program, like change
return values or skip instruction calls.

Two different styles of instrumentation exist, one is source instrumentation,
and one is binary instrumentation, where so-called instrumenter-calls are either
added pre-compilation to the source of the program or later added to the binary
context of the program. The first style might bring the advantage that it is
easier to apply and the structure of the program is better known to the
instrumenter, calls for performance checks over functions are done by adding
timer calls at the entry and exit points. However, binary instrumentation also
has its perks. For example, users do not require the source of the program,
which might not be available in proprietary programs. Also, recompilation of the
program is not needed if the code for the instrumentation changes. Given enough
debug information in the binary the instrumentation information might not differ
much from the source-based one. With binary instrumentation it is also possible
to change and read register values before and after CPU instructions are called,
which might not be possible at source code level, as used registers and memory
areas are unknown to the instrumenter at this point.

\subsubsection{Intel Pin}

Pin~\cite{pintool} is a proprietary, dynamic, binary instrumentation framework
developed and released by Intel, who supply it for free. The framework provides
a large number of API calls which abstracts most of the work done by Pin and
gives easy access to binary analysis. As Pin is a binary instrumentation
framework, the source of the to be analysed program is not needed, and the
program does it need to be recompiled if the instrumentation tool changes. The
framework allows to log and modify the runtime of a program, whereas it is no
problem to log register values or change those before or after desired
instruction calls or even inject code to be run before given calls. Programs
instrumenting a binary are called pintools, the framework provides their API
calls for C++, but also bindings for Python can be found in
\texttt{python-pin}~\cite{pythonpin}.

In their paper, Luk~\etal~\cite{pintool} describe the system of Pin and
how the written pintool is combined with the application in more detail. When
instrumenting a program, three binaries are being used. One is the application
to instrument, one is the compiled pintool, and one is Pin itself. Pin can be
seen as a userland virtual machine. When the instrumentation starts, Pin is
loaded into the address space of the application, then the \texttt{ptrace}
system-call is used to capture the program\textquotesingle s behaviour. When Pin
has initialised itself the tool overtakes the entry point of the program, or
the current program counter if it was attached to a running application. It
then uses its just-in-time (JIT) compiler to continue the execution. They state
that their approach of loading Pin has various advantages over other
instrumentation tools, as those mostly use \texttt{LD\_PRELOAD}, which would not
work on statically linked binaries and also by using an additional library, the
address space would be changed from a normal execution.

The JIT compiler inside Pin is used to inject the instrumentation API into the
application to allow the pintool to use it. The code is loaded into a preserved
code-cache and then executed. Pin always tries to execute as many instructions
as possible without interfering it for debugging. That is why they state,
execution of the code happens one trace at a time. Whereas a trace is either
till a control change happens, like branching, a call or return, or a
pre-defined API call is required for the instruction. With this approach, they
gain better performance than other instrumentation techniques. In their paper,
they also state that the JIT is not the bottleneck but the instrumenting code
inside the application. That is the reason why Pin tries to optimise the
injected code as good as possible. As their first routine, they inline all
analysis functions. This is faster as no calls happen, which would cause
registers to be used for the arguments. Most other debugging tools use the
\texttt{eflags} CPU registers, which is not ideal, as storing and restoring
these flags is time-consuming, and an extra stack has to be used, as the
original stack is not allowed to be modified. Pin detects in their JIT if the
\texttt{eflag} content is needed later on, and only if it is, saving routines
are called. Another optimisation for their framework is the API call
\texttt{IPOINT\_ANYWHERE}, which specifies that the analysis call can happen
anywhere in the instrumentation context, which allows better optimisation of the
instruction order.

\section{Permission Model in Linux-based Systems}

The POSIX standard~\cite{posix} defines many properties for operating systems,
it includes a description of permissions for users and their groups and how
they are set. For practical reasons, a user can be part of multiple groups. A
unique integer ID per user and group provides identification to the system. The
POSIX-defined permission structure applies to files, which can have properties
like readable, writable, executable. On creation, the user can set these
properties separately for the owner, the group of the owner and others. POSIX
also allows users with special privileges to ignore such permission checks, or
change them. Such users are called super-users.

Besides permission for files most Linux-based systems also provide permission
checks for processes and their owners. For example, only processes owned by a
super-user might be allowed special syscalls. In most systems, only one
super-user exists, namely \texttt{root}. Usually, users do not want to login
another time if they need super-user permissions for a task. POSIX compliant
operating systems, therefore, ship tools such as \texttt{su} or the more
advanced \texttt{sudo}. \texttt{su} stands for switch user and allows a user to
change the current context to another user or execute a command with the other
user\textquotesingle s permissions, providing the correct password of the other
user. \texttt{sudo} includes the same functionality as \texttt{su}.
Additionally, it has a better management system in the background and given the
correct permissions the user does not need to know the other
user\textquotesingle s password to execute commands as the target user.

\subsection{\texttt{setuid} Binary Property}

\texttt{setuid} stands for ``set user identity''~\cite{ogroupsetuid}. It allows
an executable to change its user ID on execution. An additional bit in the file
permissions represents the \texttt{setuid} property, namely \texttt{S\_ISUID}.
This property allows a user to run a program with the permissions of the
file\textquotesingle s owner. Tools like \texttt{su} and \texttt{sudo} need this
as they require the permission to change the current context. It is also
practical to allow normal users the usage of tools like \texttt{ping} or
\texttt{ip}, without allowing them to use \texttt{sudo}. We refer to the GNU
libc documentation for more details~\cite{libcpermission}.

Whereas the \texttt{setuid} property might seem practical, it also brings
security risks. A bug allowing code execution in such a program allows an
attacker to execute that code with the permission of another user. Therefore,
the number of \texttt{setuid} binaries should be low and programs having that
property should be well audited.

\subsection{\texttt{chroot}}

\texttt{chroot}~\cite{ogroupchroot} stands for ``change root directory''. One
can either use it via the system call or the wrapper program. A \texttt{chroot}
call changes the root directory for the running process. Meaning, the
environment is changed, and the program runs inside a completely different
system. It is not possible to access files from outside the new environment nor
to interact with processes running in the old environment. The tool is either
used for testing purposes, to create packages in clean environments, to check
compatibility, as the chrooted system could be a different system, or for
sandboxing and privilege separation. 

\subsection{Pluggable authentication module (PAM)}

PAM is used by most Linux-based operating systems to authenticate users. PAM is
just a collection of low-level authentication schemes which should provide a
general API to allow authentication without the developer knowing what exactly
is used in the background. According to the PAM manpage for Linux~\cite{pamman},
modules are split into four groups:

\begin{itemize}
\item The account group verifies if the user is permitted access and if the
password is not expired.
\item The authentication group verifies the user\textquotesingle s credentials.
These could be given in various ways, like a common password input, a
fingerprint, or as a combination of multiple ways.
\item The password groups is used to update and manage authentication
mechanisms. Modules from this group are usually connected with those from the
authentication group.
\item The session group provides ways to manage permissions for users after
authentication. It ensures the user stays authorised and also modules could
implement ways to log out users or check session termination from other sources.
\end{itemize}

Usually, Linux-based operating systems use the default \texttt{pam\_unix.so} for
all groups. This module looks up credentials from the \texttt{/etc/passwd} file.
Additionally, modules as \texttt{pam\_deny.so} and \texttt{pam\_permit.so} are
used to provide access right management for single services per default.

\section{Connecting Computers}

People are exchanging data between devices for several decades now. The Internet
started when scientists found a way to have universities share research results
between computers over the back then used Advanced Research Projects Agency
Network (ARPANET).  The ARPANET already used the TCP/IP protocol and showed
requirements for future interconnection networks. With easier access to
computers, more universities, companies and private households wanted to be
connected and exchange data. Many protocols were introduced with the creation of
the Internet, where Hypertext protocols are shared in the World Wide Web,
electronic mail is used to transmit messages, and file sharing became common.
With the growing amount of users, security in the network and the connected
devices became more relevant every day. Therefore, protocols to ensure security
for the participants became standard and regularly improved.

\subsection{Webserver}

Web servers provide users with content on the Internet, mostly referred to as
the World Wide Web (WWW). There are many different software implementations for
such servers available, most of them being open source and free, such as
Apache~\cite{apacheweb} and \texttt{nginx}~\cite{nginxweb}. Web servers use the
Hypertext Transfer Protocol (HTTP) to handle requests and deliver the content to
the client. Web servers usually deliver websites encoded in Hypertext Markup
Language (HTML) but can also deliver all kind of files including JavaScript (JS)
and Content Style Sheet (CSS), which are rendered together with HTML by web
browsers on the client.

A client requests content from the server by sending a Uniform Resource Locator
(URL). The server then looks up the given path in the URL and answers the
request by sending the desired file or an error, if one occurs during the
handling of the request. As not all requests and answers should publicly
visible, web servers adapted TLS for security and privacy reasons to provide
content over HTTPS. The client, most of the time a web browser, can thereby
verify the server\textquotesingle s identity and apply encryption to the
connection if required.

\subsubsection{HTTP - Basic Access Authentication (BA)}

To allow only authorised users to access specified paths on a web server HTTP
added access-control to the protocol described in RFC 7617~\cite{rfc7617}. This
technique makes use of a header field in HTTP, namely \texttt{WWW-Authenticate}.
Where RFC 3986~\cite{rfc3986} deprecates the functionality to transmit
credentials inside the requesting URL. BA is a straightforward access-control
technique and does not protect the transmitted credentials. Therefore, it is
used together with HTTPS at most times. The client has to transmit the
credentials with every request, and there is no logout function as in other
techniques using cookies or session identifiers.

\subsubsection{Access control in Apache and \texttt{nginx}}

Basic Access Authentication is used by these two web servers in the default
configuration. Apache makes use of \texttt{ht} files, which are an extension to
the usual server configuration for users with limited permissions, see
their documentation~\cite{apacheauthdoc} for further details. Apache introduced
\texttt{htpasswd}-files to make use of BA, which stores the user credentials.
\texttt{nginx} later adapted these files for their web server implementation.
The \texttt{htpasswd}-files keep the data either in plaintext or hashed. To
create these files Apache provides the tool
\texttt{htpasswd}~\cite{htpasswddoc}. This tool uses MD5 hashing per default (as
of version 2.2.18), but it is possible to change the hashing algorithm to
something more secure, like bcrypt. On a request including BA, the web server
will look up the \texttt{htpasswd}-file to verify given credentials and then
return the requested content or an error according to RFC 7231~\cite{rfc7231}.

\subsection{Transport Layer Security}

Transport Layer Security (TLS) describes a collection of cryptographic
protocols. It is a standard proposed by the Internet Engineering Task Force
(IETF), RFC 5246~\cite{rfc5246} describes this standard in detail. The IETF
regularly updates the TLS standard by adding new protocols and removing old,
seen as insecure, ones.

TLS provides a secure connection between various devices, usually clients and a
server. The connection holds the following properties:
\begin{itemize}
  \item Authenticated identities are provided by public-key cryptography. This
is needed for clients to make sure they communicate with the correct server. TLS
certificates provide the ability to prove the identity of a server. Protocols
use the certificates as they contain the public key of the server.
  \item Private connections between the parties are provided by symmetric
cryptography. The participants in a connection have to agree on how they
exchange the key for the symmetric encryption.
  \item Reliability is provided by the so-called message authentication code
(MAC), which prevents undetected alteration or losses during TLS communication.
\end{itemize}

As TLS is just a collection of protocols, the parties have to agree on which one
they use for each part of the connection, which happens during the TLS
Handshake. A common name for a TLS connection protocol would be
\texttt{ECDHE-RSA-AES256-GCM-HMAC-SHA1}, which contains all three protocols used
for the connection. Whereas \texttt{ECDHE-RSA} provides the key exchange,
\texttt{AES256-GCM} is the  symmetric cypher during the communication and
\texttt{HMAC-SHA1} is used to check the integrity of the transmitted data.

TLS is commonly used in web servers to guarantee user\textquotesingle s safety,
but also applications like Virtual Private Networks (VPNs) and Secure Shell
(SSH) make use of protocols and cyphers described in the TLS standard. The most
commonly used TLS implementation is OpenSSL~\cite{opensslweb}.

\subsection{Cryptographic Nonces}

In the early days of encrypted communication, replay-attacks were a common
problem, as Syverson showed in his taxonomy of replay attacks in
1994~\cite{replaytax}. Following protocols introduced nonces to resolve this
issue. Cryptographic nonces are one-time, often pseudorandom, numbers used for
each packet sent. They are combined with the key so that the encryption differs
for each packet. This technique makes replay-attacks harder.

In most cases, nonce reuse is a problem for the protocol and results in breakage
of the encryption of future or past packages. Hence, most protocol standards
define a nonce as a pseudorandom, non-repeating or unpredicted
value~\cite{noncegeneral}. It is also possible to use the nonce as a counter
which has a random start value and increases for each packet sent. However, also
current protocols and technologies still face problems with nonces, like
Vanhoef~\etal showed in their work in 2017, that it is possible to force nonce
reuse in WPA2~\cite{wpanoncereuse}.

\subsection{Advanced Encryption Standard (AES)}

The Advanced Encryption Standard is a block cypher proposed to the US National
Institute of Standards and Technology (NIST) by Vincent Rijmen and Joan Daemen
in 1999~\cite{aesproposal}. In 2001 NIST released AES as a specification for
encryption of electronic data. It is the successor of the Data Encryption
Standard (DES). AES is also part of ISO/IEC 18033-3~\cite{iso18033}, the ISO
standard related to security techniques and encryption algorithms.

\subsubsection{Design of AES}

AES is a block cypher, it is applied to data blocks, as the plaintext and a
given key are permutated together to create the ciphertext.

\subsubsection{Substitution Boxes}

So-called S-boxes are a significant part of the AES algorithm. In a simplified
view, they can be seen as a lookup table which tells which value has to be
replaced which what other value. These lookup tables are designed in a way that
no mathematical analysis is possible based on linear or differential
dependencies between values. In AES the Rijndael S-box is used, the
lookup-table describing this S-box can be seen in table~\ref{tab:sbox}. A
lookup in the S-box for a byte works as the least significant nibble (four
bytes) determines the column to be used, and the most significant nibble
determines the row. For example, the entry byte of $0x3C$ would output $0xEB$,
using the given S-box.

\begin{table}[]
\centering
\begin{tabular}{c|cccccccccccccccc}
   & 00 & 01 & 02 & 03 & 04 & 05 & 06 & 07 & 08 & 09 & 0a & 0b & 0c & 0d & 0e &
0f \\ \hline
00 & 63 & 7c & 77 & 7b & f2 & 6b & 6f & c5 & 30 & 01 & 67 & 2b & fe & d7 & ab &
76 \\
10 & ca & 82 & c9 & 7d & fa & 59 & 47 & f0 & ad & d4 & a2 & af & 9c & a4 & 72 &
c0 \\
20 & b7 & fd & 93 & 26 & 36 & 3f & f7 & cc & 34 & a5 & e5 & f1 & 71 & d8 & 31 &
15 \\
30 & 04 & c7 & 23 & c3 & 18 & 96 & 05 & 9a & 07 & 12 & 80 & e2 & eb & 27 & b2 &
75 \\
40 & 09 & 83 & 2c & 1a & 1b & 6e & 5a & a0 & 52 & 3b & d6 & b3 & 29 & e3 & 2f &
84 \\
50 & 53 & d1 & 00 & ed & 20 & fc & b1 & 5b & 6a & cb & be & 39 & 4a & 4c & 58 &
cf \\
60 & d0 & ef & aa & fb & 43 & 4d & 33 & 85 & 45 & f9 & 02 & 7f & 50 & 3c & 9f &
a8 \\
70 & 51 & a3 & 40 & 8f & 92 & 9d & 38 & f5 & bc & b6 & da & 21 & 10 & ff & f3 &
d2 \\
80 & cd & 0c & 13 & ec & 5f & 97 & 44 & 17 & c4 & a7 & 7e & 3d & 64 & 5d & 19 &
73 \\
90 & 60 & 81 & 4f & dc & 22 & 2a & 90 & 88 & 46 & ee & b8 & 14 & de & 5e & 0b &
db \\
a0 & e0 & 32 & 3a & 0a & 49 & 06 & 24 & 5c & c2 & d3 & ac & 62 & 91 & 95 & e4 &
79 \\
b0 & e7 & c8 & 37 & 6d & 8d & d5 & 4e & a9 & 6c & 56 & f4 & ea & 65 & 7a & ae &
08 \\
c0 & ba & 78 & 25 & 2e & 1c & a6 & b4 & c6 & e8 & dd & 74 & 1f & 4b & bd & 8b &
8a \\
d0 & 70 & 3e & b5 & 66 & 48 & 03 & f6 & 0e & 61 & 35 & 57 & b9 & 86 & c1 & 1d &
9e \\
e0 & e1 & f8 & 98 & 11 & 69 & d9 & 8e & 94 & 9b & 1e & 87 & e9 & ce & 55 & 28 &
df \\
f0 & 8c & a1 & 89 & 0d & bf & e6 & 42 & 68 & 41 & 99 & 2d & 0f & b0 & 54 & bb &
16
\end{tabular}
\caption{The S-box for AES as a lookup-table for single bytes. The least
significant nibble determines the column the most significant one the row.}
\label{tab:sbox}
\end{table}

The permutation of plaintext is done as the algorithm applies some steps to the
data. These steps are repeated in so-called rounds. Depending on the key length
the used the number of rounds differs. Usually, in AES the steps are:

\begin{itemize}
\begin{samepage}
\item \texttt{AddRoundKey} - In this step, the round-key is added to the block
via the XOR operation. Figure~\ref{fig:addroundkey} shows how the byte blocks
are combined to craft the new encrypted one.
\item \texttt{SubBytes} - In the substitute bytes step, every byte inside the
block is substituted with the equivalent S-box entry. This is an alphabetic
replace encryption. Figure~\ref{fig:subbytes} illustrates how the function
works on an example block.
\item \texttt{ShiftRows} - Blocks can be seen as two-dimensional arrays. In this
phase, the rows of the arrays are shifted to the left. The number of shifts
depends on the block-size and the number of rows. The overflowing rows are
re-added on the right side. Figure~\ref{fig:shiftrows} shows how this function
is applied to the matrix-blocks.
\item \texttt{MixColumns} - In this phase, the data in the columns get changed.
This happens with a matrix multiplication where each column-vector is multiplied
with a constant matrix to form a new vector. The multiplication happens in the
modulo of a Galois-Field polynomial, making this operation irreversible.
Figure~\ref{fig:mixcolumns} shows this function application. The multiplication
happens modulo $x^4 + 1$ and the polynomial of $c(x) = 3x^3 + x^2 + x + 2$ is
multiplied.
\end{samepage}
\end{itemize}

\begin{figure}
  \centering
  \begin{tikzpicture}[remember picture]
\matrix(m1)[matrixstyle]
{
$a_{0,0}$ & $a_{0,1}$ & $a_{0,2}$ & $a_{0,3}$  \\
$a_{1,0}$ & $a_{1,1}$ & $a_{1,2}$ & $a_{1,3}$  \\
$a_{2,0}$ & |[draw, fill=red!20]|\rn{a1}{a_{2,1}} & $a_{2,2}$ & $a_{2,3}$  \\
$a_{3,0}$ & $a_{3,1}$ & $a_{3,2}$ & $a_{3,3}$  \\
};
\matrix(km)[matrixstyle, below of=m1, yshift=-3cm]
{
$k_{0,0}$ & $k_{0,1}$ & $k_{0,2}$ & $k_{0,3}$  \\
$k_{1,0}$ & $k_{1,1}$ & $k_{1,2}$ & $k_{1,3}$  \\
$k_{2,0}$ & |[draw, fill=red!20]|\rn{k1}{k_{2,1}} & $k_{2,2}$ & $k_{2,3}$  \\
$k_{3,0}$ & $k_{3,1}$ & $k_{3,2}$ & $k_{3,3}$  \\
};
\node (xorbox) [XOR, right of=m1, scale=2, xshift=1cm, yshift=-0.5cm] {};
\matrix(m2)[matrixstyle, right of=m1, xshift=5cm]
{
$b_{0,0}$ & $b_{0,1}$ & $b_{0,2}$ & $b_{0,3}$  \\
$b_{1,0}$ & $b_{1,1}$ & $b_{1,2}$ & $b_{1,3}$  \\
$b_{2,0}$ & |[draw, fill=red!20]|\rn{b1}{b_{2,1}} & $b_{2,2}$ & $b_{2,3}$  \\
$b_{3,0}$ & $b_{3,1}$ & $b_{3,2}$ & $b_{3,3}$  \\
};
  \end{tikzpicture}
  \begin{tikzpicture}[overlay, remember picture]
\draw [arrow] (a1) -- (xorbox.west);
\draw [arrow] (k1) -- (xorbox.south);
\draw [arrow] (xorbox.east) -- (b1);
  \end{tikzpicture}
  \caption{Design of the \texttt{AddRoundKey} function inside AES, according to
the AES propsal~\cite{aesproposal}. Each byte in the $a$-block is XORed with
one byte of the key-block and placed in the $b$-block.}
  \label{fig:addroundkey}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture}[remember picture]
\matrix(m1)[matrixstyle]
{
$a_{0,0}$ & $a_{0,1}$ & $a_{0,2}$ & $a_{0,3}$  \\
$a_{1,0}$ & $a_{1,1}$ & $a_{1,2}$ & $a_{1,3}$  \\
$a_{2,0}$ & |[draw, fill=red!20]|\rn{b1}{a_{2,1}} & $a_{2,2}$ & $a_{2,3}$  \\
$a_{3,0}$ & $a_{3,1}$ & $a_{3,2}$ & $a_{3,3}$  \\
};
\node (sbox) [segblock, right of=m1, xshift=2cm] {S-box};
\matrix(m2)[matrixstyle, right of=m1, xshift=5cm]
{
$b_{0,0}$ & $b_{0,1}$ & $b_{0,2}$ & $b_{0,3}$  \\
$b_{1,0}$ & $b_{1,1}$ & $b_{1,2}$ & $b_{1,3}$  \\
$b_{2,0}$ & |[draw, fill=red!20]|\rn{b2}{b_{2,1}} & $b_{2,2}$ & $b_{2,3}$  \\
$b_{3,0}$ & $b_{3,1}$ & $b_{3,2}$ & $b_{3,3}$  \\
};
  \end{tikzpicture}
  \begin{tikzpicture}[overlay, remember picture]
\draw [arrow] (b1) -- (sbox);
\draw [arrow] (sbox) -- (b2);
  \end{tikzpicture}
  \caption{Design of the \texttt{SubBytes} function inside AES, according to
the AES propsal~\cite{aesproposal}. Each byte in the $a$-block is looked up in
the S-box from table~\ref{tab:sbox} and placed in the $b$-block.}
  \label{fig:subbytes}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture}[remember picture]
\matrix(m1)[matrixstyle]
{
$a_{0,0}$ & $a_{0,1}$ & $a_{0,2}$ & $a_{0,3}$  \\
$a_{1,0}$ & $a_{1,1}$ & $a_{1,2}$ & $a_{1,3}$  \\
$a_{2,0}$ & $a_{2,1}$ & $a_{2,2}$ & $a_{2,3}$  \\
$a_{3,0}$ & $a_{3,1}$ & $a_{3,2}$ & $a_{3,3}$  \\
};
\matrix(m2)[matrixstyle, right of=m1, xshift=5cm]
{
$a_{0,0}$ & $a_{0,1}$ & $a_{0,2}$ & $a_{0,3}$  \\
$a_{1,1}$ & $a_{1,2}$ & $a_{1,3}$ & $a_{1,0}$  \\
$a_{2,2}$ & $a_{2,3}$ & $a_{2,0}$ & $a_{2,1}$  \\
$a_{3,3}$ & $a_{3,0}$ & $a_{3,1}$ & $a_{3,2}$  \\
};
  \end{tikzpicture}
  \begin{tikzpicture}[overlay, remember picture]
\draw [arrow] (m1) -- (m2);
  \end{tikzpicture}
  \caption{Design of the \texttt{ShiftRows} function inside AES, according to
the AES propsal~\cite{aesproposal}. Each row is shifted according to their
Y-axis index.}
  \label{fig:shiftrows}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture}[remember picture]
\matrix(m1)[matrixstyle]
{
$a_{0,0}$ & $a_{0,1}$ & |[draw, fill=red!20]|\rn{b11}{a_{0,2}} & $a_{0,3}$  \\
$a_{1,0}$ & $a_{1,1}$ & |[draw, fill=red!20]|\rn{b12}{a_{1,2}} & $a_{1,3}$  \\
$a_{2,0}$ & $a_{2,1}$ & |[draw, fill=red!20]|\rn{b13}{a_{2,2}} & $a_{2,3}$  \\
$a_{3,0}$ & $a_{3,1}$ & |[draw, fill=red!20]|\rn{b14}{a_{3,2}} & $a_{3,3}$  \\
};
\node (cxmul) [segblock, right of=m1, xshift=2cm] {$\otimes c(x)$};
\matrix(m2)[matrixstyle, right of=m1, xshift=5cm]
{
$b_{0,0}$ & $b_{0,1}$ & |[draw, fill=red!20]|\rn{b21}{b_{0,2}} & $b_{0,3}$  \\
$b_{1,0}$ & $b_{1,1}$ & |[draw, fill=red!20]|\rn{b22}{b_{1,2}} & $b_{1,3}$  \\
$b_{2,0}$ & $b_{2,1}$ & |[draw, fill=red!20]|\rn{b23}{b_{2,2}} & $b_{2,3}$  \\
$b_{3,0}$ & $b_{3,1}$ & |[draw, fill=red!20]|\rn{b24}{b_{3,2}} & $b_{3,3}$  \\
};
  \end{tikzpicture}
  \begin{tikzpicture}[overlay, remember picture]
  \draw [arrow] (b12) -- (cxmul);
  \draw [arrow] (cxmul) -- (b22);
  \end{tikzpicture}
  \caption{Design of the \texttt{MixColumns} function inside AES, according to
the AES propsal~\cite{aesproposal}. Each column-vector in the $a$-block is
multiplied with a polynomial matrix $c(x)$ and then placed in the $b$-block.}
  \label{fig:mixcolumns}
\end{figure}

The algorithm applies these steps in the following order:

\begin{enumerate}
\begin{samepage}
\item Key expansion - In this phase the round keys are generated to be later
applied in the repeating steps. Those keys are derived from the main key. The
round keys have the same size as the blocks used.
\item Pre-round - In the first round only the key is added, no S-box lookup or
shifting is applied.
\begin{itemize}
\item \texttt{AddRoundKey}
\end{itemize}
\item Encryption rounds - These are the default rounds, which can be applied
multiple times. The number of repetitions depends on the size of key.
\begin{itemize}
\item \texttt{SubBytes}
\item \texttt{ShiftRows}
\item \texttt{MixColumns}
\item \texttt{AddRoundKey}
\end{itemize}
\item Last-round - The last round  does not include the call to
\texttt{MixColumns}, it was removed because of implementation reasons and with
the ommiting the security was still provided, this matter is discussed in the
Rijnadael design document~\cite{aesproposal}.
\begin{itemize}
\item \texttt{SubBytes}
\item \texttt{ShiftRows}
\item \texttt{AddRoundKey}
\end{itemize}
\end{samepage}
\end{enumerate}

Decryption happens in the same manner as encryption with just the order of
function calls reversed. The only thing is that a different S-box is used, but
that one can be derived from the encryption one. All other operations usually
have an inverse implementation or use the same, as for example, XOR calls do not
differ in that regard.

AES is used in most encryption schemes and protocols. Like WPA2 adapts it to
secure wireless networks. SSH uses it to have secure communication between
computers. Intel CPUs even include AES instructions in their architectures,
which speeds up encryption and decryption for libraries as OpenSSL.

\subsubsection{Attacks on AES}

Since AES was released, researchers and attackers targeted the cypher and tried
to find weak spots in it. Most of these attacks targeted a smaller number of
rounds than the algorithm describes. In 2011, Bogdanov~\etal~\cite{bicliqueaes}
showed an attack against AES which targets all of the rounds. Their attack
weakens the security of AES slightly, where for 128-bit key recovery the
computational complexity drops to $2^{126.1}$ and for 256-bit keys to
$2^{254.4}$. As these are still very high complexities, the attack does not
affect real-world usages of AES.

Way more common than such cryptoanalysis attacks on AES are attacks targeting
software and hardware implementations. For example,
O'Flynn~\etal~\cite{aespowerboot} showed an attack on AES in the bootloader.
Whereas most attacks target 128 bits, they showed one against 256. They use a
Correlation Power Analysis (CPA) on a bootloader which loads an AES-256-CBC
encrypted firmware. Besides recovering the full key, they also achieve recovery
of the initialisation vector. Another hardware side-channel attack was made by
Lo~\etal~\cite{sboxpoweranal}. They target the implementation of the S-box used
in AES-128 besides using CPA, they also use differential power analysis (DPA).
In their work they compare both methods as they try to recover cypher keys from
the \texttt{AddRoundKey} and \texttt{SubBytes} function calls. Their work was
done with widespread hardware, the Arduino Uno, making it possible for more
people to learn from such attacks, as they can replicate it with cheap
hardware. With an increasing number of side-channel attacks on software and
hardware, vendors increased the key-size of AES in their implementations. In
their work, Neve~\etal~\cite{sidecomplex}, show that increasing the key-size
does not increase the complexity of side-channel attacks in the same manner.
They show for cache-based side-channel attacks an upgrade from 128 bits to 256
only increases the complexity of an attack by a factor of 6 to 7.

\subsection{AES-GCM}

Galois Counter Mode (GCM) in combination with Advanced Encryption Standard (AES)
provides encryption and simultaneously performs message
authentication~\cite{gcm, gcmnist}. Figure~\ref{fig:aesgcm} shows the schematic
of AES-GCM. Where we use close to the same notation as BÃ¶ck~\etal in their
paper~\cite{gcmnonceattack}:

\begin{itemize}
\begin{samepage}
  \item[$CNT_i$] The $i$-th counter block, computed using the concatenation of
the $IV$ and the counter value $cnt$, $cnt = (i+1) \mod{2^{32}}$, to achieve 128
bits. $CNT_0 = IV \parallel 0 ^{31} \parallel 1$. With $IV$ being the
initialisation vector with a size of 96 bits.
  \item[$E_k$] AES encryption with symmetric key $k$.
  \item[$a \oplus b$] XOR operation between $a$ and $b$.
  \item[$P_i$] The $i$-th plaintext block.
  \item[$C_i$] The $i$-th ciphertext block.
  \item[$mult_H$] Multiplication $H \cdot X$ in Galois Field GF($2^{128}$),
with the irreducible polynomial $g = g(x) = x^{128} + x^{7} + x^{2} + x + 1$.
  \item[$A$] Authenticated data.
  \item[$len(X)$] Bit-length of $X$.
  \item[$a \parallel b$] Concatenation of $a$ and $b$.
  \item[$TAG$] Authentication tag.
\end{samepage}
\end{itemize}

\begin{figure}
  \centering
  \begin{tikzpicture}
    \node (cnt0) [block]  {$CNT_0$};
    \node (cnt1) [block, right of=cnt0, xshift=3cm]  {$CNT_1$};
    \node (cnt2) [block, right of=cnt1, xshift=3cm]  {$CNT_2$};
    \node (enc0) [function, below of=cnt0, yshift=-0.5cm] {$E_k$};
    \node (enc1) [function, below of=cnt1, yshift=-0.5cm] {$E_k$};
    \node (enc2) [function, below of=cnt2, yshift=-0.5cm] {$E_k$};
    \node (xor11) [XOR, below of=enc1, scale=2, yshift=-0.5cm] {};
    \node (xor12) [XOR, below of=enc2, scale=2, yshift=-0.5cm] {};
    \node (pt1) [block, left of=xor11, xshift=-1cm] {$P_1$};
    \node (pt2) [block, left of=xor12, xshift=-1cm] {$P_2$};
    \node (ct1) [block, below of=xor11, yshift=-0.5cm] {$C_1$};
    \node (ct2) [block, below of=xor12, yshift=-0.5cm] {$C_2$};
    \node (xor21) [XOR, below of=ct1, scale=2, yshift=-0.25cm] {};
    \node (xor22) [XOR, below of=ct2, scale=2, yshift=-0.25cm] {};
    \node (mult1) [function, left of=xor21, xshift=-1.5cm] {$mult_H$};
    \node (mult2) [function, right of=xor21, xshift=1cm] {$mult_H$};
    \node (mult3) [function, below of=xor22, yshift=-0.5cm] {$mult_H$};
    \node (auth) [block, below of=mult1, yshift=-0.5cm] {$A$};
    \node (xor31) [XOR, below of=mult3, scale=2, yshift=-0.25cm] {};
    \node (len) [block, left of=xor31, xshift=-1cm] {$len(A) || len(C)$};
    \node (mult4) [function, below of=xor31, yshift=-0.5cm] {$mult_H$};
    \node (xor41) [XOR, below of=mult4, scale=2, yshift=-0.25cm] {};
    \node (tag) [block, right of=xor41, xshift=1cm] {$TAG$};

    \draw [arrow] (cnt0) -- (cnt1);
    \draw [arrow] (cnt1) -- (cnt2);
    \draw [arrow] (cnt0) -- (enc0);
    \draw [arrow] (cnt1) -- (enc1);
    \draw [arrow] (cnt2) -- (enc2);
    \draw [arrow] (enc0) |- (xor41);
    \draw [arrow] (enc1) -- (xor11);
    \draw [arrow] (enc2) -- (xor12);
    \draw [arrow] (pt1) -- (xor11);
    \draw [arrow] (pt2) -- (xor12);
    \draw [arrow] (xor11) -- (ct1);
    \draw [arrow] (xor12) -- (ct2);
    \draw [arrow] (ct1) -- (xor21);
    \draw [arrow] (ct2) -- (xor22);
    \draw [arrow] (mult1) -- (xor21);
    \draw [arrow] (auth) -- (mult1);
    \draw [arrow] (xor21) -- (mult2);
    \draw [arrow] (mult2) -- (xor22);
    \draw [arrow] (xor22) -- (mult3);
    \draw [arrow] (mult3) -- (xor31);
    \draw [arrow] (len) -- (xor31);
    \draw [arrow] (xor31) -- (mult4);
    \draw [arrow] (mult4) -- (xor41);
    \draw [arrow] (xor41) -- (tag);
  \end{tikzpicture}
  \caption{Schematic of AES-GCM. Whereas the $E_k$ blocks apply
AES-encryption with the key $k$. The $mult_H$ blocks apply a multiplication in
the Galois-field. $A$ represents a block of authenticated data, which is added
to the MAC $TAG$ but not encrypted.}
  \label{fig:aesgcm}
\end{figure}

Following steps describe how AES-GCM is used to encrypt data:

\begin{enumerate}
\begin{samepage}
  \item The initialisation vector $IV$ of 96 bits is generated.
  \item The counters $CNT_i$ of 128 bits are generated by $CNT_i = IV \parallel
cnt$, where $cnt = (i + 1) \bmod{2^{32}}$, for $i \in\{0 .. n\}$ and $n$
represents the number of plaintext blocks.
  \item The ciphertexts $C_i$ are generated by encrypting the counter values
$CNT_i$ and XORing it to the plaintext $P_i$, as $C_i = P_i \oplus E_k(CNT_i)$.
\end{samepage}
\end{enumerate}

To get the authentication tag $TAG$, the $GHASH$ function,
$GHASH(H, A, C) = X_{m+n+1}$, is applied. Where, $H$ is the hash key, computed
by encrypting 128 zero bits with the AES block cypher, $C$ is the ciphertext,
and $A$ is non-encrypted but authenticated data. Figure~\ref{fig:aesgcm}
shows how the $GHASH$ is computed for one block of authenticated data and two
blocks of plaintext. Following steps describes the computation of the $TAG$:

\begin{enumerate}
\begin{samepage}
  \item The Galois field multiplication is applied to the block of
authenticated data. $R_0 = mult_H(A_1)$.
  \item The result of this is XORed to the first ciphertext. The output
of this XOR is again multiplied in the Galois field. $R_1 = mult_H(R_0 \oplus
C_1)$.
  \item This output is then XORed to the second ciphertext and again
multiplied. $R_2 = mult_H(R_1 \oplus C_2)$
  \item Then the length of the authenticated data and the ciphertext is
added. $R_3 = mult_H(R_2 \oplus (len(A) \parallel len(C)))$
  \item The hash key is added in the end to form the tag $TAG$. $TAG = H \oplus
mult_H(R_3)$.
\end{samepage}
\end{enumerate}

The authenticated tag $TAG$ can therefore be found by solving the polynomial
$g(X) = A_{1}X^{m+n+1} + \cdots + A_{m}X^{n+2} + C_{1}X^{n+1} + \cdots +
C_{n}X^2 + LX + S$, as $g(H) = TAG$. Where $L$ is the combined length of $A$
and $C$, and $S$ is the nonce plus the counter. We refer to the NIST
standard about AES-GCM~\cite{gcmnist} for more details.

\subsubsection{OpenSSL source code}

The libraries souce code is structured in a way that each part of a
cypher is implemented on its own. For our thesis and analysis we want to
refer to version \texttt{1.1.0g} of OpenSSL~\cite{opensslsource}. The AES-GCM
code part is located at \texttt{crypto/evp/e\_aes.c}. The structures used by the
cypher are defined in \texttt{crypto/evp/evp\_locl.h},
\texttt{crypto/include/internal/evp\_int.h} and \texttt{ssl/ssl\_locl.h}. The
file \texttt{crypto/evp/evp\_lib.c} shows the implementation for the creation of
the cypher data.

\subsubsection{AES-GCM Nonce Reuse Attack}

Joux~\cite{NISTGCMcomment} calls his attack against GCM a ``forbidden attack''
because in cryptography the uniqueness of nonces is seen as given.
BÃ¶ck~\etal~\cite{gcmnonceattack} describe how relevant nonce reuse still is in
real-world application and why clear definitions for nonces in protocols are
needed. 

We look at the simplified attack of Joux\textquotesingle s
comment~\cite{NISTGCMcomment}, described by BÃ¶ck~\etal~\cite{gcmnonceattack}.
Assuming that the same nonce is used for two messages, consisting of just a
single ciphertext block and no authenticated data. Also, we note that all
values besides $S$ are known to the attacker:

\[g_1(X) = C{_{1}X^2 \oplus L_1X \oplus S}\]
\[g_2(X) = C{_{2}X^2 \oplus L_2X \oplus S}\]

We add the tag $TAG$ to each polynomial to get $g_i'(X)$ as follows:

\[g_1'(X) = C{_{1}X^2 \oplus L_1X \oplus S \oplus T_1}\]
\[g_2'(X) = C{_{2}X^2 \oplus L_2X \oplus S \oplus T_2}\]

Knowing that $g_x'(H) = 0$, because $g_x(H) = T_x$, we combine those two
formulas to following polynomial:

\[g_{1+2}'(X) = (C_{1} + C_{2})X^2 + (L_1 + L_2)X + T_1 + T_2\]

This polynomial results in $0$ at the point $h$, $g_{1+2}'(H) = 0$.
Therefore, an attacker can calculate a list of possible solutions for $H$.
With an increasing number of nonce reuses the number of possible values for $H$
decreases.

\section{Software-based Microarchitectural Attacks}

Attacks against desktop computers and mobile devices have been widespread over
the last couple of years. As programmers and vendors became more aware of
classic issues in such systems, attackers moved to other layers to find bugs and
security issues. One of these other layers is side-channel information leakage.
Like Kelsey~\etal~\cite{kelsey1998side} showed how little information was needed
to break cryptographic cypher implementations. They used time, processor flags,
and power consumption as their sources to break various implementations. But not
only cryptography was affected by side-channel attacks. As
Weinberg~\etal~\cite{weinberg2011still} showed, it is also possible to gather
information about a users browser history by using side-channels.

\subsection{Low Level Performance Measuring}

Many side-channel attacks are based on timings. Like, predicting what a machine
currently does because of the time needed to compute something. To measure
timings in a high resolution, researchers want as precise time stamps as
possible.

\subsubsection{Native Timing Measurement}

In native code, researchers use the highly precise timestamp counter (TSC)
provided on x86 architectures since the first Pentium CPU~\cite{intelsys}. The
CPU stores this time stamp in a 64-bit register. The TSC counts the number of
execution cycles since the last resetting of the CPU. As CPUs tend to change
their clock frequency during runtime, the TSC might deliver different results
depending on the current frequency. Therefore, vendors changed the behaviour of
the TSC so that it is increased in the same way as it would on maximum
frequency. The instruction \texttt{RDTSC} provides the TSC\textquotesingle s
value in the
\texttt{EDX:EAX} CPU registers. With out-of-order execution, problems occurred
with measuring timings, as the CPU read the TSC at the wrong time. This issue
can be resolved by calling serialising instructions such as \texttt{CPUID} or
use the serialising \texttt{RDTSCP} instruction, which reads the timestamp
counter to \texttt{EDX:EAX} and the processor ID into \texttt{EXC}.

\subsubsection{JavaScript Timing Measurement}

Calls to the \texttt{RDTSC} instruction are generally only possible in native
code, but as Gruss~\etal~\cite{memdedupjs} and Lipp~\etal~\cite{keytimejs}
showed, it is also possible to receive highly precise timestamps in JavaScript.
In their work, we see another countermeasure by browser vendors defeated by
researchers. In JavaScript, a time stamp function exists, namely
\texttt{performance.now()}. As researchers started to use the JavaScript time
stamp counter in their side-channel attacks, browser vendors were forced to
reduce the resolution of the reported value, which was nanosecond precise in the
past and is only milliseconds precise now. Browser vendors thought with this
countermeasures JavaScript is more secure, but researchers like
Gruss~\etal~\cite{memdedupjs} and Vila~\etal~\cite{loophole} showed that there
are ways to bring back highly-precise timestamps in the area of single-digit
microseconds.

\subsection{Caches}

Vendors of processors introduced the principle of caching to memory management
to speed up access times for memory. Hardware components closer to the computing
core holding a part of memory for future requests reply faster. In modern CPUs,
there are three or four levels of caching applied. As shown in
figure~\ref{fig:intelcache}, the design in Intel Kaby Lake assigns caches for
level one and two (L1, L2) to each core and the cores share the level three
cache (L3) between each other. L1 caches can be split up in a data and an
instruction cache. There might also be an L0 cache for micro-operations, as it
is implemented in the Intel Haswell architecture.

Caches are updated on each memory access. Inclusive caching is a form of
managing the caches in a way where data stored on one level is also at lower
levels. The architecture defines if the CPU uses inclusive caches and which ones
are inclusive to others. The memory management part of the processor needs to
validate memory all the time to make sure memory accesses do not get wrong data
from other cache levels or the DRAM. Computation at rates of about four
gigahertz would not be possible without such caching mechanisms.

We refer to the Intel manual~\cite{intelsys} chapter 11.1 for more details about
cache management in modern CPUs.

\begin{figure}
  \centering
  \begin{tikzpicture}
    % Level 1 caching blocks
    \node (C1L1) [block] {$C_1-L_1$};
    \node (C2L1) [block, right of=C1L1, xshift=1.5cm] {$C_2-L_1$};
    \node (C3L1) [block, right of=C2L1, xshift=1.5cm] {$C_3-L_1$};
    \node (C4L1) [block, right of=C3L1, xshift=1.5cm] {$C_4-L_1$};
    % Level 2 caching blocks
    \node (C1L2) [block, below of=C1L1, yshift=-1cm] {$C_1-L_2$};
    \node (C2L2) [block, right of=C1L2, xshift=1.5cm] {$C_2-L_2$};
    \node (C3L2) [block, right of=C2L2, xshift=1.5cm] {$C_3-L_2$};
    \node (C4L2) [block, right of=C3L2, xshift=1.5cm] {$C_4-L_2$};
    % Level 3 block
    \node (L3) [level3block, below of=C2L2, yshift=-1cm, xshift=1.25cm] {$L_3$};
    % Connections
    \draw [doublearrow] (C1L1) -- (C1L2);
    \draw [doublearrow] (C2L1) -- (C2L2);
    \draw [doublearrow] (C3L1) -- (C3L2);
    \draw [doublearrow] (C4L1) -- (C4L2);
    \draw [doublearrow] (C1L2) -- (0,-3.6cm);
    \draw [doublearrow] (C2L2) -- (2.5cm,-3.6cm);
    \draw [doublearrow] (C3L2) -- (5cm,-3.6cm);
    \draw [doublearrow] (C4L2) -- (7.5cm,-3.6cm);
    % Memory Controler and DDR3
    \node (MC) [mcblock, below of=L3, yshift=-1cm] {Memory Controller};
    \node (DDR31) [block, below of=MC, yshift=0.2cm, xshift=-1cm] {DRAM};
    \node (DDR32) [block, below of=MC, yshift=0.2cm, xshift=1cm] {DRAM};
    \draw [doublearrow] (L3) -- (MC);
  \end{tikzpicture}
  \caption{Schematic of Intel Kaby Lake caching architecture. Whereas $C_x$
represents each processing core and $L_x$ the level of the cache. Requests are
sent from the processing core to each memory storage, the one finding the
desired memory first will answer before the others. Caches get updated with
each request.} \label{fig:intelcache}
\end{figure}

\subsubsection{Cache Attacks}

Cache Attacks are part of microarchitectural attacks, where scientists have
shown that caching also introduces security risks to systems.
Gruss~\etal~\cite{gruss2015cache} showed how caches provide information by
measuring memory access-timings. They also showed how to automate attacks on
last-level caches. Knowledge gained from this kind of attack has also played
a role in the exploitation of the Rowhammer bug or other hardware bugs, such as
Meltdown~\cite{meltdown} and Spectre~\cite{spectre}.

\subsubsection{FLUSH+RELOAD}

This cache attack is a low noise, L3 cache side-channel attack, released
by Yarom~\etal~\cite{flushreload}. The attack provides a possibility of leaking
information about memory accesses on shared pages between two processes. As the
attack targets the last-level cache, it does not require the two processes to
run on the same core. Pages are either shared when actively mapped with such
property or mapped as shared with copy-on-write when a process is forked. One
process is referred to as spy and the other as victim. The attack consists of
three phases.

\begin{enumerate}
\begin{samepage}
\item The spy uses the instruction \texttt{clflush} to remove the given address
from the CPU caches.
\item The spy waits a pre-defined time.
\item The spy accesses the flushed memory with reading access. If it takes less
than a defined time, the victim has accessed the memory in this timespan.
\end{samepage}
\end{enumerate}

With this attack, certain information can be leaked to the spy processes. For
example, in cryptographic algorithms, accessed code lines could leak the private
key to the attacker.

\subsubsection{FLUSH+FLUSH}

This is a cache attack designed by Gruss~\etal~\cite{flushflush} to circumvent
detection of cache attacks such as FLUSH+RELOAD. This attack only uses only the
\texttt{clflush} instruction and no memory accesses. With this idea, the attack
bypasses cache attack detection by performance counters. Instead of the three
phases in FLUSH+RELOAD, this attack only has one phase. The spy process gains a
memory address to target, will call \texttt{clflush} on it and measure the time
it took to flush the cache. There are two possible outcomes for this instruction
call:

\begin{itemize}
\begin{samepage}
\item The CPU has not loaded the memory into the cache and the \texttt{clflush}
results in a cache-miss. Therefore the instruction can return immediately and
not flush any other cache levels.
\item The victim process has caused the CPU to load the memory into the cache.
Therefore the flush needs to be applied to all cache levels, which results in
longer execution time for the \texttt{clflush} instruction.
\end{samepage}
\end{itemize}

With measuring the execution time of \texttt{clflush}, the spy process can again
tell when a victim has accessed targeted memory, as the timing will be higher in
such cases. Besides the leakages similar to the FLUSH+RELOAD results,
Gruss~\etal also built a side-channel with this principle to transmit
information between two processes, according to their paper~\cite{flushflush},
they reach a transmission speed of $496$ kilobytes per second.

\subsection{Impact of Microarchitectural Attacks}

With growing knowledge on the CPU and caching level, researchers found major
security vulnerabilities in modern CPUs. Attacks like Meltdown, Spectre and
Foreshadow were reported by many newspapers and influenced the awareness of
microarchitectural attacks by the general public a lot.

\subsubsection{Meltdown}

Lipp~\etal~\cite{meltdown} presented an attack targeting the memory isolation
between the operating system\textquotesingle s kernel and the userland memory
and shows how the CPU\textquotesingle s out-of-order execution can lead to a
leak of information from kernel memory to a user program. Meltdown makes use of
a side-effect of this execution method. The CPU wants to use as many computing
units as possible. It will execute code as soon as all dependencies are
resolved. So inside a program computing could happen ahead of other code parts.
In a simplified view, the attacker accesses kernel memory, which the CPU reads
out-of-order, the CPU executes other code and receives an exception because of
the disallowed memory access, which already happened before. This execution
order still makes accesses to caches and DRAM as it would normally access the
memory. Therefore the cache could be used to transmit the information. Meltdown
makes use of the Flush+Reload cache attack, as they use the read kernel memory
to flush certain cache lines. Access times to memory then tells which cache
lines the program cleaned. Hence it leaks the kernel memory content. Lipp~\etal
propose the KAISER countermeasure as a fix for this vulnerability. This work by
Gruss~\etal~\cite{kaiserpaper} is used to separate kernel and user memory
addresses. Their proof-of-concept implements a complete separation of the two
memory spaces inside the Linux kernel, which makes sure no information about the
kernel execution is leaked to the userspace.

\subsubsection{Spectre}

Kocher~\etal~\cite{spectre} presented this attack also targeting information
leakage by features modern CPUs provide. The attack gained its name form
exploiting speculative execution which makes use of the branch-prediction unit
inside the CPU. The CPU executes code which maybe needs execution later on,
the CPU executes multiple branches and results are either thrown away or
reported to the program later on. Spectre uses this behaviour to leak
information to an attacker by using side-channels inside the predicted code.
The attack breaks multiple security measures which would usually be in place in
modern operating systems. In their work, the researchers show multiple variants
of exploiting the different behaviours caused by speculative execution. For
example see the code in listing~\ref{lst:spectre1}, as shown in their work. The
speculative execution executes the array access before the if condition check.
The \texttt{array1[x]} can point to kernel memory, for example, returning some
secret, then the \texttt{array2} is accessed at a page which refers to the
secret. This can later be checked again using cache accesses timings using
Prime+Probe or Flush+Reload again. The attacker can see the outcome of the
accesses after the condition check, even if it fails, as it was executed
speculatively. Other variants make use of similar behaviours of the CPU. The
researchers also show how the Spectre attack can be applied using JavaScript.

\begin{minipage}{\linewidth}
\begin{lstlisting}[style=CStyle,
                   caption={Code for Spectre variant one, showing a
conditional branch example, as seen by the work done by
Kocher~\etal~\cite{spectre}.},
                   label={lst:spectre1}]
if(x < array1_size)
  y = array2[array1[x] * 4096];
\end{lstlisting}
\end{minipage}

\subsubsection{Foreshadow}

An attack by van Bulck~\etal~\cite{foreshadow}, targeting the Guard eXtension
(SGX) in current Intel CPUs. The researchers use already known speculative
execution bugs to gain knowledge of secret memory used inside the SGX
environment which is transferred to the CPU\textquotesingle s cache. The attack
works similar to the Meltdown attack, but it is more challenging because of the
SGX and its additional memory protection layers. With this attack, the security
promises on the hardware layer made by Intel are broken. Countermeasures
against this attack cannot be applied to the kernel layer. Intel needs to patch
their microcode in order to prevent the Foreshadow attack.

\subsection{Rowhammer}

Technology is steadily changing and improving, with vendors of computer parts
being forced to produce cheaper and better hardware every year. For DRAM this
resulted in less quality management and a declining number of proper checks for
hardware faults. However, the industry produced smaller and faster memory chips.
These new chips need less space and energy to store electric load, which
represents data. Years after vendors introduced increased densities in DRAM to
the market, researchers like Kim~\etal~\cite{rowhammergeneral} found a critical
bug in the chip design. With the little space and energy stored it was possible
to use changes of charges in memory rows to interfere with data of nearby cells.
This interference resulted in a change of bits in other memory rows.

This behaviour and bug then caused researchers to dig deeper into the
possibilities, and several attacks were found based on the Rowhammer bug. For
example, Google\textquotesingle s ``Project Zero''~\cite{projectzerorow} showed,
it is possible to use Rowhammer for privilege escalation and sandbox escapes.
Van der Veen~\etal~\cite{drammer} state that this attack not only affects
desktop computers but also memory in mobile devices. In their work,
Gruss~\etal~\cite{flipinthewall} show that flipping bits in memory has further
risks than previously thought and that Rowhammer defences need a general
overhaul. They state that the only solution is to fix the hardware.
Gruss~\etal~\cite{rowhammerjs} also showed that it is possible to trigger
bitflips from JavaScript.

\subsubsection{Design of Dynamic Random-Access Memory (DRAM)}

Random-access memory is designed to store information as bits. A simplified view
of RAM is a circuit containing amplifiers, one-bit storage cell matrix, an
address decoder and buffers for input and output of the memory.
Figure~\ref{fig:DRAMscheme} shows a simple schematic of a DRAM module. The
storage matrix is a two-dimensional array of one-bit storage cells. Accesses to
DRAM do not happen per bit but per row. The size of a row depends on the DRAM
design, but it is usually a multiple of the word size. In modern systems, rows
are larger than 64 kilobytes. The CPU sends requests for memory to the
controller which then translates it to a sequence, where first the column
entries to read are set via the bit lines and then the desired row via the word
lines. DRAM chips contain many memory arrays and controllers which a data bus
connects with the processor.

A one-transistor (1-T) memory cell, as seen in figure~\ref{fig:1Tstorage}, is
used to store a single bit. To access memory, first, the desired bit lines are
charged with $\frac{V_{CC}}{2}$, then the word line is also charged to switch on
the transistors. An amplifier is then used to detect changes of charge on the
bit lines. A discharged capacitor would cause the voltage on the bit line to
drop, whereas a charged capacitor would raise the voltage to its load. If a
charged or discharged capacitor represents true or false for the bit state,
depends on the design of the DRAM chip. Writing the bits works in a similar way,
where the bit lines of a row are either set to $V_{CC}$ or $0$, and then the
word line is used to switch on the capacitors on the row, making the capacitors
change their charge to the level of the connected bit line.

The capacitor used for the 1-T cell will either lose load over time or on
reading access. Therefore, these capacitors need to be periodically refreshed.
The JEDEC standard~\cite{jedec} sets the refresh cycle to a 64-millisecond
interval. For a usual chip with $8192$ rows, this means a refresh happens all
$7,8$ microseconds. The easiest way to do this is by reading a bit and writing
the response back to it. In the DRAM design, it is possible to refresh each row
at a time with a RAS without the following CAS. With this refresh cycle, the
chip needs to hold the information which was the last refreshed row. Some DRAM
designs, therefore, implement a so-called row refresh pointer holding this
information.

\begin{figure}
  \centering
  \begin{circuitikz}
  \ctikzset{label/align = smart}
  \draw
  (0,0) to[short, l_=$bit\ line$] (0,-4)
  (-1,-1) to[short, l^=$word\ line$] (4,-1)
  (1,-2) node[nmos, rotate=-90] (mos) {}
  (mos.gate) node[anchor=north] {}
  (mos.drain) node[anchor=east] {}
  (mos.source) node[anchor=west] {}
  ;
  \draw
  (mos.gate) to[short] (1,-1)
  (mos.source) to[short] (0,-2)
  ;
  \draw
  (mos.drain) to[short] (2,-2)
  (2,-3) node[rground] (A) {}
  node[anchor=west, yshift=-0.4cm, xshift=0.2cm] {} to[pC, l_=$C$] (2,-2)
  ;
  \end{circuitikz}
  \caption{Schematic of a 1-T memory storage element. The capacitor $C$ is used
to store information. The transistor makes it possible to access the storage by
applying charge on the connected lines. Depending on the voltage used it is a
read or a write access.}
  \label{fig:1Tstorage}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture}
  %blocks
  \node (arr) [memarray]  {Memory Array};
  \node (rowdec) [memfunc, left of=arr, rotate=90, yshift=1.5cm] {Row decoder};
  \node (amp) [memfunc, below of=arr, yshift=-1.5cm] {Amplifier};
  \node (coldec) [memfunc, below of=amp, yshift=-0.5cm] {Column decoder};
  \node (memcon) [memcon, left of=amp, align=center, xshift=-2cm,
                  yshift=-0.5cm] {Memory \\ Controller};
  \node (buf) [memfunc, right of=arr, rotate=90, yshift=-1.5cm] {Data Buffer};
  %connections
  \draw [arrow] (amp) -| (buf);
  % memcon -> rowdec
  \draw [arrow] (-2.5,-2) -- (-2.5,-1.5);
  % memcon -> coldec
  \draw [arrow] (-2,-3.75) -- (-1.5,-3.75);
  % coldec -> amp
  \draw [arrow] (-1,-3.5) -- (-1,-3);
  \draw [arrow] (1,-3.5) -- (1,-3);
  % amp -> arr
  \draw [arrow] (-1,-2) -- (-1,-1.5);
  \draw [arrow] (1,-2) -- (1,-1.5);
  \node at (0,-1.75) {bit lines};
  % rowdec -> arr
  \draw [arrow] (-2,-1) -- (-1.5,-1);
  \draw [arrow] (-2,1) -- (-1.5,1);
  \node at (-1.75,0) [rotate=90] {word lines};
  % arr <-> buf
  \draw [doublearrow] (1.5,-1) -- (2,-1);
  \draw [doublearrow] (1.5,1) -- (2,1);
  % memcon <-> buf
  \draw [arrow] (-3,-5) -- (memcon.south);
  \draw [line] (-3,-5) -- (3.5,-5);
  \draw [line] (3.5,-5) -- (3.5,0);
  \draw [arrow] (3.5,0) -- (3,0);
  \end{tikzpicture}
  \caption{Schematic of a single DRAM module. The memory controller is managing
the data buffer to fill or read the buffer via a bus connection to the CPU. The
decoders are used to make sure the correct bits are accessed. The amplifier is
used because the bit voltage differences are usually too small to change the
memory in the buffer directly.}
  \label{fig:DRAMscheme}
\end{figure}

\subsubsection{Introducing bitflips to DRAM}

Kim~\etal~\cite{rowhammergeneral} show how it is possible to cause bits to flip
inside memory by correct access patterns. They use two rows as so-called
aggressors, and one as target row. As the code in
listing~\ref{lst:rowhammercode} shows, the two aggressor rows are repeatedly
read, causing memory accesses in the DRAM. The changes in electrical charge
caused by the read accesses will by chance interfere with the loads in the
target row and change content there. The \texttt{clflush} instructions are used
to empty the memory in the used cache lines, causing the System to access the
DRAM for each read instead of getting the value out of the cache. These accesses
happen fast enough so that the refresh cycle of the DRAM cannot ensure that the
capacitors hold enough voltage.

\begin{minipage}{\linewidth}
\begin{lstlisting}[
  label={lst:rowhammercode},
  style=nasm,
  caption={Code to trigger the Rowhammer bug, as of
Kim~\etal~\cite{rowhammergeneral}. The preating accesses to the aggressor
locations cause bitflips in a third victim row. The \texttt{CFLUSH} is
neededed to make sure the DRAM rows are accessed directly. According to
Project Zero~\cite{projectzerorow} the \texttt{mfence} is not needed and even
lowered the number of flips.},]
code1a:
  mov (X), %eax
  mov (Y), %ebx
  clflush (X)
  clflush (Y)
  mfence
  jmp code1a
\end{lstlisting}
\end{minipage}

The memory addresses to be accessed to gain the rows next to each other are hard
to find. Linux provides a file, \texttt{/proc/<PID>/pagemap}, where physical
positioning is stored. Some systems allow huge pages, where two megabytes of
memory are used contiguously. These two megabytes will use more than one row,
other than normal pages with the size of four kilobytes. In the paper,
Kim~\etal~\cite{rowhammergeneral} use addresses calculated based on their
knowledge of physical address mapping used by common CPU vendors. Project
Zero~\cite{projectzerorow} show how it is possible to use Rowhammer to
gain privilege escalation on a standard \texttt{x86\_64} CPU and an escape from
a sandbox. After vendors of operating systems started to introduce
countermeasures, scientists like Gruss~\etal~\cite{rowhammerjs, flipinthewall},
came up with further, improved attacks making use of the Rowhammer bug. They
show more possible targets for flips and also show that Rowhammer is possible in
JavaScript.
%}}}

%% vim:foldmethod=expr
%% vim:fde=getline(v\:lnum)=~'^%%%%\ .\\+'?'>1'\:'='
%%% Local Variables:
%%% mode: latex
%%% mode: auto-fill
%%% mode: flyspell
%%% eval: (ispell-change-dictionary "en_US")
%%% TeX-master: "main"
%%% End:
